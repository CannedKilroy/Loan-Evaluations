{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b5a87e7-3f0f-49cd-bf5c-740bc94a1a17",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a19a03-c85e-4502-b298-c85101ef9b6e",
   "metadata": {},
   "source": [
    "## Date: OCT 10, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f4e8ea",
   "metadata": {},
   "source": [
    "-- ------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1715de5e-0908-4ff3-82ce-246ed69273ca",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7606bb8-f8a4-4c60-85ef-858832c9e883",
   "metadata": {},
   "source": [
    "This notebook cleans the data for the lending club accepted loans, then exports the data as a parquet file. Due to the size of the dataset, the csv is read in chunks, with a random sample taken from each each chunk. Only fully paid and charged off / defaulted loans are sampled as current loans hold no value in classifying the target variable. This allows us to more efficiently load the data. Those samples are merged and will become the working dataset for the duration of the project. Unnecessary and leaky features are removed inplace to be more space efficient, formatted and null values removed. Finally the dataframe size is optimized to optimize space and computation efficiency, then exported. This file will be used for the EDA notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1701fc-f92e-4bd9-a8f3-b9c891eadf34",
   "metadata": {},
   "source": [
    "### Table-of-contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bbe9b1-8cfd-4215-b9ca-6ec5d9a2b7e7",
   "metadata": {},
   "source": [
    "\n",
    "1. [Introduction](#Introduction)\n",
    "   - [Table-of-contents](#Table-of-contents)\n",
    "   - [Import-Librarys](#Import-Librarys)\n",
    "   - [Data Dictionary](#Data-Dictionary)\n",
    "   - [Define-Functions](#Define-Functions)\n",
    "   - [Load in the data](#Load-the-data)\n",
    "3. [Data Cleaning](#Data-Cleaning)\n",
    "   - [Initial Exploration](#Initial-Exploration)\n",
    "   - [Feature Pruning](#Feature-Pruning)\n",
    "   - [Explore Columns to drop](#Explore-Columns-to-drop)\n",
    "   - [Dataframe Null Values](#Dataframe-Null-Values)\n",
    "4. [Dataframe optimization](#Dataframe-optimization)\n",
    "5. [Exploratory-Data-Analysis](Exploratory-Data-Analysis)\n",
    "6. [Feature Engineering](#Feature-Engineering)\n",
    "7. [Conclusion](#Conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9252c44",
   "metadata": {},
   "source": [
    "### Import-Librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d59f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pdcast as pdc\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17654b3a-269f-4d42-a4c3-b10a8873d67a",
   "metadata": {},
   "source": [
    "### Data-Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d7734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb2aa2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pathlib is used to ensure compatibility across operating systems\n",
    "try:\n",
    "    data_destination = Path('../Data/Lending_club/Lending Club Data Dictionary Approved.csv')\n",
    "    dict_df = pd.read_csv(data_destination, encoding='ISO-8859-1')\n",
    "    display(dict_df.iloc[:,0:2])\n",
    "except FileNotFoundError as e:\n",
    "    print(e.args[1])\n",
    "    print('Check file location')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625c720f-9f8a-490c-9bcc-5c84aa28e660",
   "metadata": {},
   "source": [
    "#### Define-Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcadaa11-002e-41ea-a2a9-925a4e5a3e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_emp_length(employment_length:str):\n",
    "    '''\n",
    "    Takes in employment length and returns an int for mapping\n",
    "\n",
    "    :param employment_length: The employment length to be mapped\n",
    "    :type employment_length: str\n",
    "\n",
    "    :return: The int employment length should be mapped to\n",
    "    :type return: int\n",
    "    '''\n",
    "    if employment_length == '< 1 year':\n",
    "        return 0.5\n",
    "    elif employment_length == '10+ years':\n",
    "        return 10\n",
    "    elif 'years' or 'year' in employment_length:\n",
    "        return int(employment_length.split()[0])\n",
    "    elif employment_length == '0':\n",
    "        return 0\n",
    "    else:\n",
    "        return employment_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bf7abd-8cce-4099-b670-73456af6d1a8",
   "metadata": {},
   "source": [
    "When initially loading in the dataset, Pandas raised a DtypeWarning over mixed datatypes within various columns. Setting low_memory = False while breaking the CSV into chunks allows Pandas to load an entire chunk before guessing the data types. When the script to scrape the data dictionary is finished, the data dict can then be passed in instead of relying on pandas. The mixed_data_types function is stilled called as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e3869-47e5-4e04-b89b-ebbe8a4da053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed_data_types(df:pd.DataFrame) -> bool:\n",
    "    '''\n",
    "    Takes in a dataframe and checks for columns with mixed data types\n",
    "    If none are found return False, else True\n",
    "    \n",
    "    :param df: The dataframe to be checked\n",
    "    :type df: obj\n",
    "    :return bool: True if found, false if none were found\n",
    "    :type return: bool\n",
    "    '''\n",
    "    \n",
    "    #loop through each column\n",
    "    for column in df:\n",
    "\n",
    "        #filter out int datatype coming from Nan and get unique data types\n",
    "        unique_types = df[column].dropna(inplace=False).apply(type).unique()\n",
    "\n",
    "        #if there are more than 1 datatype in a column\n",
    "        if unique_types.size > 1:\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869b1603-308a-4ddb-9766-594feac1245a",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a42820-7bc4-4790-81d5-ef808754cd70",
   "metadata": {},
   "source": [
    "Due to the size of the dataset, it is read in chunks. After each chunk is read and checked for mixed data types, it is randomly sampled and then placed within a list. Only fully paid and /defaulted and charged off loans are taken, as current loans including late or in grace period loans do not hold any value in target variable prediction. This is done when loading in the data otherwise it becomes too large for memory. It is also more efficient. The different samples are then combined into a single sample representative of the whole dataset. EDA will be performed on this single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30d90c2-bd23-4f8a-9a9e-c3ce8acbaab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 5*100000\n",
    "sample_size =  100000\n",
    "random_state = 11\n",
    "\n",
    "assert sample_size < chunk_size, f\"Cannot take a sample of {sample_size} rows out of {chunk_size} rows\"\n",
    "\n",
    "print(f'Chunk size: {chunk_size} rows')\n",
    "print(f'Rows to be sampled: {sample_size} rows')\n",
    "\n",
    "\n",
    "sampled_dataframes = []\n",
    "try:\n",
    "    \n",
    "    #path to the data. Should be under Data/Lending_club\n",
    "    data_destination = Path('../Data/Lending_club/accepted_2007_to_2018Q4.csv')\n",
    "\n",
    "    #split the csv into chunks and iterate over each chunk\n",
    "    #set low_memory to false to force pandas to load entire columns before guessing data type\n",
    "    with pd.read_csv(data_destination, chunksize=chunk_size, low_memory = False) as reader:\n",
    "        \n",
    "        for count,chunk in enumerate(reader):\n",
    "            if mixed_data_types(df=chunk) == True:\n",
    "                raise Exception(\"Mixed data types found\")\n",
    "\n",
    "            #define a list that includes only finished loan statuses\n",
    "            finished_loan_status = ['Fully Paid',\n",
    "                                    'Charged Off',\n",
    "                                    'Does not meet the credit policy. Status:Fully Paid',\n",
    "                                    'Does not meet the credit policy. Status:Charged Off',\n",
    "                                    'Default']\n",
    "                        \n",
    "            #filter the dataframe for loans that are finished or null\n",
    "            filtered_chunk = chunk.loc[chunk['loan_status'].isin(finished_loan_status) | chunk['loan_status'].isnull()]\n",
    "            \n",
    "            #sample the filtered df and append to list\n",
    "            sampled_df = filtered_chunk.sample(n=sample_size, random_state=random_state)\n",
    "            sampled_dataframes.append(sampled_df)\n",
    "            \n",
    "            print(f\"{count} sampled dataframe shape: {sampled_df.shape}\")\n",
    "        print('Finished')\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(e.args[1])\n",
    "    print('Check file name and location')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e.args[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf972fc-423c-4d4e-86fa-af16729ef0b3",
   "metadata": {},
   "source": [
    "There are no duplicate datatypes within any columns. The random samples can be combined into a single sample dataframe. This sample will be used as the working dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd48046-7d83-4aa7-86b1-66d7349cb708",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_accepted_df = pd.concat(sampled_dataframes, ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee15a8a-69e1-4da0-9062-fe55b7fea232",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd08d900-4c61-4f28-a187-b678483e509b",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f56533c-c67d-44e3-a7ad-45481efa64bc",
   "metadata": {},
   "source": [
    "### Initial Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec421c-5836-454e-b624-d2b2c81627a8",
   "metadata": {},
   "source": [
    "***Display the first 5 rows*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920eba48-983f-4642-a67a-8fe5c3e8f4cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_accepted_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56636bd-8420-4315-a4e0-2b80a1321198",
   "metadata": {},
   "source": [
    "***Dataframe shape***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bdc4ee-55cf-421a-8c3e-c01cda0f401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, columns = sample_accepted_df.shape\n",
    "print(f'Dataframe rows: {rows}')\n",
    "print(f'Dataframe columns: {columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c7f0b4-be64-481e-b037-ce120f9a3468",
   "metadata": {},
   "source": [
    "***Dataframe info***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e19999f-ad78-47e5-b0fc-58d1611b1d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_accepted_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e0d141-e367-4544-ac05-ed512a73befb",
   "metadata": {},
   "source": [
    "Of the 151 columns, 113 are float64 and 38 are objects. The dataframe takes up approximatly 580 MB.\n",
    "Note:\n",
    "- The numeric columns are all float64 and the object columns. These columns can be optimized later to save memory space and decrease computation time by changing the datatypes.\n",
    "- There is no datetime column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c1c760-6652-45d4-943c-ab25d0d6e3ac",
   "metadata": {},
   "source": [
    "***Describe Dataframe***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c918cb-205c-4c27-bffb-d97ae2cfea68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_accepted_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ef8472-ba68-40fd-a8f8-51ca5f361f41",
   "metadata": {},
   "source": [
    "Some key points:\n",
    "\n",
    "- Loan Amount\n",
    "  \n",
    "    - Average Loan Amount is ~ 15,000 USD with a standard deviation of 9240 USD, having a max of 40,000 USD and minimum of 500 USD. This follows LendingClubs  policies for minimum and maximum loan amounts.\n",
    "\n",
    "- Funded amount\n",
    "    - Nearly identical to the loan amount\n",
    "\n",
    "- Funded amount by investors\n",
    "    - Very similar to the  funded amount\n",
    "\n",
    "- Interest Rate\n",
    "    - The interest rates are quite high. An average of 13%, with a minimum of 5.3% and a maximum of 31%.\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a5a990-33e3-40be-a84c-f19f4a3370a1",
   "metadata": {},
   "source": [
    "***Null Values***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a5e843-b569-4752-896e-7cd3829839c8",
   "metadata": {},
   "source": [
    "Some rows are fully NaN values, aside from the id. This will cause issues when we try to inspect each column later. So we will drop `id` and the NaN rows, along with any other irrelevant columns including:  \n",
    "- member_id\n",
    "- url for the loan\n",
    "- LC policy code\n",
    "- title (information is already found under purpose)\n",
    "- initial_list_status (what market it was listed under)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbb1d11-5589-422a-bc4e-afd3fd4d753a",
   "metadata": {},
   "source": [
    "We will define a list to keep track of the columns we have dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7474c5e-a8b6-4dec-9273-22a157527cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_columns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51389a0c-e59b-4fac-8fc5-e6e07b2d3666",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns=['id', 'member_id', 'url', 'policy_code', 'title', 'initial_list_status']\n",
    "\n",
    "# append the columns to drop\n",
    "dropped_columns.extend(drop_columns)\n",
    "\n",
    "sample_accepted_df.drop(columns=drop_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27f4508-eb08-4980-8053-800c968c6674",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_rows = sample_accepted_df.isnull().all(axis=1).sum()\n",
    "print(f\"Number of Null rows: {null_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde43a87-7961-4f4e-a616-ff5f10ca104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows that are all Nan\n",
    "sample_accepted_df.dropna(how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de04222e-f762-48ed-85db-21c570a6cdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_rows = sample_accepted_df.isnull().all(axis=1).sum()\n",
    "print(f\"Number of Null rows: {null_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fb7c08-1e26-4bdd-a78e-79e679ee9517",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea0b31-7071-4b24-acf6-b8ba08c80ab2",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6facfa-7c85-4248-80f9-73526181f28d",
   "metadata": {},
   "source": [
    "### Feature Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1756e8a9-5175-4722-9cb7-05ac6fb29350",
   "metadata": {},
   "source": [
    "We will exclude any leaky features, non relevant features and any features that were not present in the original loan application. This will be done by grouping the features together to better explain why they are being dropped. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c4573a-8b2f-4bb2-ba4c-70924ca8d8e6",
   "metadata": {},
   "source": [
    "#### ***Irrelevant columns***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594259a2-bb55-4e9f-83c9-2fec98d87d57",
   "metadata": {},
   "source": [
    "***Secondary Applicants Information***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456c2bd4-a2dc-41b0-bf73-d2e83882e390",
   "metadata": {},
   "source": [
    "The columns for the secondary applicants are largely nulls, so we will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202031af-caa6-407e-8fbd-a08470e3b450",
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls_percent = (sample_accepted_df['sec_app_mort_acc'].isnull().sum()/sample_accepted_df.shape[0]*100)\n",
    "print('Percentage of null rows for secondary applicants: ', nulls_percent.round(2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadcd1a5-d253-433a-80c7-6dadf5c02e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_accepted_df['application_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af11918-5123-4012-a7e2-fa39db75c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the index of the loans where application_type is a joint application\n",
    "rows_to_remove = sample_accepted_df.loc[sample_accepted_df['application_type'] == 'Joint App'].index\n",
    "\n",
    "# drop the loans\n",
    "sample_accepted_df.drop(rows_to_remove, inplace=True)\n",
    "\n",
    "# drop the related columns\n",
    "drop_columns = ['revol_bal_joint', 'sec_app_fico_range_low', \n",
    "                'sec_app_fico_range_high', 'sec_app_earliest_cr_line',\n",
    "                'sec_app_inq_last_6mths', 'sec_app_mort_acc',\n",
    "                'sec_app_open_acc', 'sec_app_revol_util', \n",
    "                'sec_app_open_act_il', 'sec_app_num_rev_accts', \n",
    "                'sec_app_chargeoff_within_12_mths', 'sec_app_collections_12_mths_ex_med',\n",
    "                'sec_app_mths_since_last_major_derog',\n",
    "                'verification_status_joint', 'dti_joint',\n",
    "                'annual_inc_joint']\n",
    "\n",
    "# append the columns to drop\n",
    "dropped_columns.extend(drop_columns)\n",
    "sample_accepted_df.drop(columns=drop_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fca638-fe1a-4c1d-b0c5-dda745dfc09c",
   "metadata": {},
   "source": [
    "We will still keep the flag of whether the application was a joint or individual application since there are no nulls and the information could be useful in our analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcb2528-04a2-4bf8-9e8b-35684f218961",
   "metadata": {},
   "source": [
    "***Hardship Loans***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84627eb9-a026-45f0-9c79-deed21f04a60",
   "metadata": {},
   "source": [
    "Hardship loans add 15 columns of complexity, are largely nulls and leak the loan outcome. We will drop these columns and loans if they exist in our dataset, and limit our analysis to non hardship loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7494db37-7e22-42bf-b9d3-b239b3a8b5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch the value counts for the for the hardships flags\n",
    "hardships = sample_accepted_df['hardship_flag'].value_counts()\n",
    "display(hardships)\n",
    "\n",
    "#if there are loans with the yes hardship flag\n",
    "if 'Y' in hardships:\n",
    "    #get the count of hardship loans\n",
    "    yes_hardship_count = hardships.iloc[1]\n",
    "    print(f'The hardship loans represent only {(yes_hardship_count/sample_accepted_df.shape[0])*100}% of the dataset')\n",
    "\n",
    "    #get the index of the hardship loans\n",
    "    rows_to_remove = sample_accepted_df.loc[sample_accepted_df['hardship_flag'] == 'Y'].index\n",
    "\n",
    "    #drop the loans\n",
    "    sample_accepted_df.drop(rows_to_remove, inplace=True)\n",
    "\n",
    "    #check the rows have been dropped\n",
    "    assert sample_accepted_df['hardship_flag'].value_counts().shape[0] == 1\n",
    "    print('Hardship loans and associated columns have been dropped')\n",
    "\n",
    "else:\n",
    "    print('There are no hardship loans.')\n",
    "    \n",
    "drop_columns = ['hardship_flag', 'hardship_type',\n",
    "                'hardship_reason', 'hardship_status',\n",
    "                'hardship_amount', 'hardship_start_date',\n",
    "                'hardship_end_date', 'deferral_term',\n",
    "                'hardship_length', 'hardship_dpd',\n",
    "                'hardship_loan_status', 'payment_plan_start_date',\n",
    "                'orig_projected_additional_accrued_interest', 'hardship_payoff_balance_amount',\n",
    "                'hardship_last_payment_amount']\n",
    "\n",
    "# append the columns to drop\n",
    "dropped_columns.extend(drop_columns)\n",
    "sample_accepted_df.drop(columns = drop_columns, inplace=True)\n",
    "print('Hardship columns have been dropped')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da596ba-5ab4-404b-bbec-17512fb74d52",
   "metadata": {},
   "source": [
    "***Employee Title***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d66d8a-f335-4730-a3d8-3e250c90b8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_emp_titles = sample_accepted_df['emp_title'].nunique()\n",
    "print(f'Number of unique employment titles: {unique_emp_titles}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47852c1-265e-4d71-a24d-b3ea23a3613d",
   "metadata": {},
   "source": [
    "There are too many unique Employee titles to attempt any sort of grouping or encoding for now. In the future we could use NLP or an external API to group the Employee Title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858a35c2-a353-42c8-97d7-a8c37ac68ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the columns to drop\n",
    "dropped_columns.append('emp_title')\n",
    "\n",
    "sample_accepted_df.drop(columns = 'emp_title', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b591cd9-2d31-4f79-b4b2-3a37037549e6",
   "metadata": {},
   "source": [
    "***Loan Status***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381b39a4-194a-44af-b8f3-4b60d6306067",
   "metadata": {},
   "source": [
    "Any current loans have already been dropped when reading in the data. We can now finish grouping the completed loans.\n",
    "\n",
    "More information on the loan status's can be found here:  \n",
    "https://www.lendingclub.com/help/investing-faq/what-do-the-different-note-statuses-mean  \n",
    "https://www.fintechnexus.com/policy-code-2-loans-lending-club/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd57575f-8b7f-4bba-8996-93410ab022cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_accepted_df['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea29433-eb73-4a00-9966-27eda0b151fb",
   "metadata": {},
   "source": [
    "The \"Does not meet the credit policy\" means when the loans were made under a different credit card policy, that does not meet the current policy. This has affect on the loans themselves, so they can be grouped with their counter parts. Charged off and Defaulted can also been grouped together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ce01bb-eae2-4b0c-a737-c6d5547c0713",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_mapping = {\n",
    "    \"Fully Paid\": \"Fully Paid\",\n",
    "    \"Does not meet the credit policy. Status:Fully Paid\": \"Fully Paid\",\n",
    "    \"Does not meet the credit policy. Status:Charged Off\": \"Charged Off/Default\",\n",
    "    \"Charged Off\": \"Charged Off/Default\",\n",
    "    \"Default\": \"Charged Off/Default\",\n",
    "}\n",
    "\n",
    "#map the loans\n",
    "sample_accepted_df['loan_status'] = sample_accepted_df['loan_status'].map(status_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0796ad-1907-4689-ad57-822aba616c1f",
   "metadata": {},
   "source": [
    "Check the mapping has worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9249f16-102a-4d66-8b47-9bbef101b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_accepted_df['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deef39a-1730-461e-ac46-076d026c37b0",
   "metadata": {},
   "source": [
    "The mapping was successful, we are not left with only successful and failed loans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53532cc-5ad0-460d-9429-f2eea8f2a935",
   "metadata": {},
   "source": [
    "***State / Zip Code***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ef163c-161d-4062-b812-5790db588395",
   "metadata": {},
   "source": [
    "We have 2 geographical features. We will drop both of them for now as they will add too much complexity to the model. However, in the future we can perhaps use a 3rd party api and introduce mean or median income data by region, allowing us to capture some of that geographical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721aaa62-41f5-4a0b-ad1a-f8d1f06dcbaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(sample_accepted_df['addr_state'].value_counts())\n",
    "print('-'*20)\n",
    "display(sample_accepted_df['zip_code'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a87b672-318a-4f22-84d9-f10d7c4a2575",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop_columns = ['addr_state', 'zip_code']\n",
    "\n",
    "# append the columns to drop\n",
    "#dropped_columns.extend(drop_columns)\n",
    "#sample_accepted_df.drop(columns = drop_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a1ae78-5f90-4d17-bb8f-e0cd9a6123e4",
   "metadata": {},
   "source": [
    "***Description***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb13b4-0041-4f29-ad9c-b9b076825b38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_desc_titles = sample_accepted_df['desc'].nunique()\n",
    "print(f'Number of unique descriptions: {unique_desc_titles}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6802c9-e66b-4855-b6c2-70a054420f34",
   "metadata": {},
   "source": [
    "There are too many unique descriptions to create dummy variables. We can drop this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d3584d-db90-4053-8aaf-e0f79a1b150a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drop_columns = ['desc']\n",
    "\n",
    "# append the columns to drop\n",
    "dropped_columns.extend(drop_columns)\n",
    "sample_accepted_df.drop(columns = drop_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0f06b3-0a74-430b-b90d-7ce674429b2f",
   "metadata": {},
   "source": [
    "#### Leaky columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89a4b8a-061c-4356-b5d9-233338145429",
   "metadata": {},
   "source": [
    "We will remove any columns that can leak the outcome of the application ie, any data the originates after a loan has been funded or rejected.  \n",
    "The columns we have dropped so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d0daac-913b-48da-bde8-f6d044447592",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Columns dropped so far: ')\n",
    "print(dropped_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec5f562-387b-458c-8069-77f1921c0704",
   "metadata": {},
   "source": [
    "***Loan Grade***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b6dadc-5ea2-4b3a-8c0b-61ed109ca2a8",
   "metadata": {},
   "source": [
    "Loan grade is calculated after the loan is given, so we can drop both `grade` and `sub_grade`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec892f9-35b6-4ec4-b289-1e22f740cc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['grade','sub_grade']\n",
    "\n",
    "# append the columns to drop\n",
    "dropped_columns.extend(drop_columns)\n",
    "sample_accepted_df.drop(columns=drop_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd17d627-2bf6-4ddc-9b72-41fb3b44abf3",
   "metadata": {},
   "source": [
    "***Other features to drop***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac06b12-2fa8-4fc0-891e-14182c26a7f3",
   "metadata": {},
   "source": [
    "We can remove any columns that:  \n",
    "- describe payments made toward the loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779075ae-976b-43a6-89be-ff0f6e580a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns =  ['total_pymnt', 'total_rec_prncp',\n",
    "                 'total_rec_int', 'total_rec_late_fee',\n",
    "                 'last_pymnt_d', 'last_pymnt_amnt', \n",
    "                 'next_pymnt_d', 'total_pymnt_inv']\n",
    "\n",
    "# append the columns to drop\n",
    "dropped_columns.extend(drop_columns)\n",
    "sample_accepted_df.drop(columns =drop_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8810bb50-1af0-447f-86b2-e40a69f83d32",
   "metadata": {},
   "source": [
    "- describe debt collection or recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc41fa5-76a2-4534-b806-4ecbcb0062a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['collection_recovery_fee', \n",
    "                'collection_recovery_fee', 'recoveries']\n",
    "\n",
    "dropped_columns.extend(drop_columns)\n",
    "sample_accepted_df.drop(columns =drop_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feccfa3b-249d-43c3-adb2-4ad0e24650eb",
   "metadata": {},
   "source": [
    "- loan attributes post acceptance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ac62cd-af28-4d45-a673-0f5d127290be",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns=['out_prncp', 'out_prncp_inv',\n",
    "              'pymnt_plan', 'disbursement_method',\n",
    "              'last_credit_pull_d',\n",
    "              'debt_settlement_flag_date', 'settlement_term',\n",
    "              'num_tl_120dpd_2m', 'num_tl_30dpd']\n",
    "\n",
    "dropped_columns.extend(drop_columns)\n",
    "sample_accepted_df.drop(columns=drop_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceafa17-fb79-4f12-aacb-b73abdfd4faf",
   "metadata": {},
   "source": [
    "- any settlement information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cbf977-603d-4ff1-bd7a-867cb70dc9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns=['debt_settlement_flag', 'settlement_status',\n",
    "              'settlement_date', 'settlement_amount',\n",
    "              'settlement_percentage']\n",
    "\n",
    "dropped_columns.extend(drop_columns)\n",
    "sample_accepted_df.drop(columns=drop_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26760d67-091b-44f3-8a53-03b007c933d4",
   "metadata": {},
   "source": [
    "- other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b40225-3ece-4d3d-b834-53f88cbaec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['max_bal_bc', 'open_rv_24m',\n",
    "                'open_rv_12m', 'inq_fi',\n",
    "                'total_bal_il', 'inq_last_12m',\n",
    "                'open_il_24m', 'open_il_12m',\n",
    "                'open_act_il', 'total_cu_tl',\n",
    "                'open_acc_6m', 'il_util','mths_since_rcnt_il',\n",
    "                'all_util']\n",
    "\n",
    "dropped_columns.extend(drop_columns)\n",
    "sample_accepted_df.drop(columns=drop_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df46cbb-0d55-4bb7-9925-dd3c0e0f0216",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346ea73c-ad5a-4827-b3a6-5c18498e30f8",
   "metadata": {},
   "source": [
    "***Term***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba074c4-bd8b-4c18-abd1-f8200ebafda0",
   "metadata": {},
   "source": [
    "Convert from str to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22a66dd-222b-442e-8bb0-7ca63d8b805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_accepted_df['term'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a67d0a0-bdfc-4bf2-80fb-f5fdd6b131f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the \"months\" text and convert to int\n",
    "sample_accepted_df['term'] = sample_accepted_df['term'].str.extract('(\\d+)').astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289dedea-cff8-42ed-bad0-dae62d391ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_accepted_df['term'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f903bacd-1b00-4453-ab93-eeb65d5b3824",
   "metadata": {},
   "source": [
    "***Emp_Length***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860af3aa-beee-42f7-8d1f-09b323140575",
   "metadata": {},
   "source": [
    "As employment length is ordinal, we will map greater than 10 years to 10, less than 1 year to 0.5 as to differentiate it between 1 and 0, preserving that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c8693-33af-445a-b055-2cc59e367c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_accepted_df['emp_length'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666c0d1b-c748-46f7-bfc3-72769e8715b3",
   "metadata": {},
   "source": [
    "We will assume NA's as no employment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b62e80d-722d-460f-b8ad-8979807408ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_accepted_df['emp_length'].fillna(value='0',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf292c66-9a71-4371-a306-7188a9a4da19",
   "metadata": {},
   "source": [
    "Apply the mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdc877d-e9b8-4927-b8e5-38db67296a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_accepted_df['emp_length'] = sample_accepted_df['emp_length'].apply(map_emp_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4964753e-8c04-416f-b0ec-b28c7d3289bb",
   "metadata": {},
   "source": [
    "Check employment length has been updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38f1b00-2108-45ab-980c-96517af37774",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_accepted_df['emp_length'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f6f87f-24a1-45aa-bc80-993925334daa",
   "metadata": {},
   "source": [
    "### Dataframe-Null-Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5004f2e1-638d-4833-8213-63dbca23b3a6",
   "metadata": {},
   "source": [
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee32b34-8344-4d72-bd89-b00b86e4b072",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f65070-3161-4c29-a7fb-de258895e087",
   "metadata": {},
   "source": [
    "We can calculate the percentages of null values by column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd676544-e182-4021-bcb9-e965bc90cfc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(sample_accepted_df.isnull().sum()/sample_accepted_df.shape[0]*100).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d922521c-af00-4ce6-ba7b-bbfc4593eec1",
   "metadata": {},
   "source": [
    "Note how there seems to be groupings of nulls. We will explore these groupings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8087ca7a-d895-4bea-9dda-8159c36deb7c",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c81fb5-6497-4ad6-b4a9-18854a4ed8d7",
   "metadata": {},
   "source": [
    "***Explore the groupings of nulls***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c5a3d3-785e-4fde-b7a3-e8b22b9c5a42",
   "metadata": {},
   "source": [
    "We will drop the loans for columns with less than 3% nulls. With a dataset this size, a few loans won't affect our analysis. Furthermore, majority of the features within these rows are nulls. There is a noticable gap at 3% which is why we will we choose it for our cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6cefc6-d273-47ae-9edf-fa83e6cfc2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 3 #percent\n",
    "\n",
    "#get the percentages of nulls for each column \n",
    "null_percentages = (sample_accepted_df.isnull().sum() / sample_accepted_df.shape[0]) * 100\n",
    "\n",
    "#get the filtered columns\n",
    "filtered_columns = null_percentages[null_percentages < cutoff].index.tolist()\n",
    "\n",
    "#drop the loans with nulls for the filtered columns\n",
    "sample_accepted_df_cleaned = sample_accepted_df.dropna(subset=filtered_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fab6a7c-29ec-4d46-b474-adf7d109fe48",
   "metadata": {},
   "source": [
    "We will also drop any columns that are majority nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba999b89-7a16-465b-bea4-153af4cca40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cutoff = 10 #percent\n",
    "\n",
    "#get the percentages of nulls for each column \n",
    "null_percentages = (sample_accepted_df.isnull().sum() / sample_accepted_df.shape[0]) * 100\n",
    "\n",
    "#get the filtered columns\n",
    "filtered_columns = null_percentages[null_percentages > column_cutoff].index.tolist()\n",
    "dropped_columns.extend(filtered_columns)\n",
    "\n",
    "#drop the filtered columns\n",
    "sample_accepted_df.drop(columns=filtered_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8160ddc-384f-4645-9707-3f84632d34b6",
   "metadata": {},
   "source": [
    "This leaves us with the the following column nulls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26635f7d-7d60-460e-8d5f-d33fe1925e36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(sample_accepted_df.isnull().sum()/sample_accepted_df.shape[0]*100).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ba7662-1cd4-4192-823f-c112b6dca23c",
   "metadata": {},
   "source": [
    "We can now work through each grouping, starting with the samllest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd8b057-e474-4d15-9b96-e14fb4237936",
   "metadata": {},
   "source": [
    "***acc_open_past_24mths***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a833620-4e5f-4415-b505-fd4946cabe6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "null_rows = sample_accepted_df[sample_accepted_df['acc_open_past_24mths'].isnull()]\n",
    "null_rows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469ba45a-fc80-4aaf-a139-349ca3e443e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "null_rows['issue_d'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0bb45e-63d1-4a4f-a90b-86ffba10a29a",
   "metadata": {},
   "source": [
    "Notice the date for the loans. The loans made early in lendingclubs history make up the majority of nulls for the remaining columns. This is impart due to lendingclub frequently updating their api, adding new fields, while the loans that are already recorded are filled with NaN values. Since our analysis is based on the exact combination of features for a loan, it simply does not make sense to keep these loans as there is no accurate way to impute the many missing values. We can remove the associated rows or features. Although this may add some recency bias, as we are narrowing our analysis to more recent loans that may not have as varied economic conditions among other factors, we will drop the rows due to our dataset size.  \n",
    "Example:  \n",
    "https://www.fintechnexus.com/lending-club-adds-15-new-fields-and-folio-introduces-a-true-secondary-market-api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d363a1-36d1-46f4-bc21-a69b258116f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_accepted_df.dropna(subset=['annual_inc', 'total_acc', \n",
    "                                  'tax_liens', 'chargeoff_within_12_mths', \n",
    "                                  'pub_rec_bankruptcies', 'total_bal_ex_mort',\n",
    "                                  'tot_hi_cred_lim', 'avg_cur_bal', \n",
    "                                  'pct_tl_nvr_dlq', 'mo_sin_old_il_acct', 'bc_util', 'percent_bc_gt_75'\n",
    "                                 ], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6549b9e3-42ee-4664-8fed-4086995e7744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(sample_accepted_df.isnull().sum()/sample_accepted_df.shape[0]*100).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c8d36e-5af3-44d0-8a03-421724e37108",
   "metadata": {},
   "source": [
    "We have no more null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fc5270-5fff-4c12-96fa-67dba4fda1b6",
   "metadata": {},
   "source": [
    "### Dataframe Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885e664a-1f91-4ed8-b0cf-f41297206516",
   "metadata": {},
   "source": [
    "The library used to optimize the dataframe shape minimize to int8, which is not supported by parquet files. The code is left for reusability in the case someone wants to export as csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d01727d-1e3d-4bf5-b2fa-7340bfb5e0f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sample_accepted_df = pdc.downcast(sample_accepted_df)\n",
    "#print(sample_accepted_df.info())\n",
    "# Infer minimum schema for DataFrame.\n",
    "#schema = pdc.infer_schema(sample_accepted_df)\n",
    "#print(schema)\n",
    "#sample_accepted_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dcce27-9f2c-4f95-a060-07c386858855",
   "metadata": {},
   "source": [
    "Instead we will simply downcast the datatypes to int and float32 as this is more than enough precision for our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac73595-1dfd-4924-bcc5-fa34c3e07f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downcast all float columns to float32\n",
    "float_cols = sample_accepted_df.select_dtypes(include='float64').columns\n",
    "for col in float_cols:\n",
    "    sample_accepted_df[col] = pd.to_numeric(sample_accepted_df[col], downcast='float')\n",
    "\n",
    "#downcast all int columns to int32\n",
    "int_cols = sample_accepted_df.select_dtypes(include='int64').columns\n",
    "for col in int_cols:\n",
    "    sample_accepted_df[col] = sample_accepted_df[col].astype('int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9421ba10-f1d1-4d9a-89a5-9aaa4af6aea8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_accepted_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c14064-1071-406e-8555-5a3100f3c7c2",
   "metadata": {},
   "source": [
    "### Export Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0dadd8-c520-4f7d-a618-ad91aebdd84f",
   "metadata": {},
   "source": [
    "***Export the dataframe for EDA***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4206ef3-e797-45b8-bcea-a6f46cab7c73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff9d27d-4a37-4666-be89-a9d599cde864",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_destination = Path('../Data/Lending_club/eda_cleaned')\n",
    "sample_accepted_df.to_parquet(export_destination)\n",
    "print('Cleaned data to be used for EDA has been exported')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe496cf6-088e-4596-9cac-93c7638e9c7f",
   "metadata": {},
   "source": [
    "***Export the dataframe for Models***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4eb2c0-284d-43f1-b17a-7ef5bf2115a5",
   "metadata": {},
   "source": [
    "Drop any Leaky columns left over from EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a729e9-4bac-4ace-9640-9c5312a51a48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#drop leaky columns / unwanted columns\n",
    "drop_columns=['funded_amnt', 'funded_amnt_inv', 'fico_range_low', 'fico_range_high', 'last_fico_range_high', 'last_fico_range_low']\n",
    "dropped_columns.extend(drop_columns)\n",
    "sample_accepted_df.drop(columns = drop_columns, inplace=True)\n",
    "\n",
    "# drop categorical columns with too many categories for one hot encoding\n",
    "drop_columns=['issue_d', 'earliest_cr_line', 'zip_code', 'addr_state']\n",
    "dropped_columns.extend(drop_columns)\n",
    "sample_accepted_df.drop(columns = drop_columns, inplace=True)\n",
    "\n",
    "print('The final list of columns dropped : ')\n",
    "print(dropped_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b198af59-3e32-4f13-84ba-10ee7b8bd2c2",
   "metadata": {},
   "source": [
    "Map **Successful loans to 1**, and **Defaulted or Charged Off loans to 0** in our target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2333fc3e-ae45-43eb-bc2c-bc43c3dca190",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_accepted_df['loan_status'] = sample_accepted_df['loan_status'].apply(lambda x: 1 if x == 'Fully Paid' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a134e163-88de-4ab3-94d4-205620dc6275",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_destination = Path('../Data/Lending_club/model_cleaned')\n",
    "sample_accepted_df.to_parquet(export_destination)\n",
    "print('Cleaned data to be used for modelling has been exported')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c5c163-a62f-4399-8282-e4cc1190ffe2",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d494a97-07a4-4a5d-8cd0-4cd6d2a364ff",
   "metadata": {},
   "source": [
    "In this notebook, we have completed a rudimentary cleaning of the lendingclub dataset. A random sample of 500,000 rows was taken from the dataset and cleaned. We have have dealt with any missing information stemming from changes in lendingclub's api over the years, and any other NaN values. Any features that could leak the outcome of the loan, were irrelevant, or added unnecessary complexity were have also been dropped. Some rudimentary feature engineering has been conducted but this will be expanded on later. Finally, the cleaned dataset is written to a parquet file.   \n",
    "\n",
    "Note:   \n",
    "- Although the leaky features were carefully reviewed, we will check the feature weights when performing our baseline logitist regression model to confirm that some leaky features haven't been kept. \n",
    "- We have removed the earlier loans completed in lendingclubs history due to api changes and large amount of null values. This restricts our dataset to a more recent timeframe, which could introduce a recency bias, given how sensitive loans are on economic conditions over this shorter period, as will be shown in EDA.\n",
    "- Some features were kept for the sake of EDA, but will be dropped later. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a935c1-af40-4292-9db5-1b7d3b383943",
   "metadata": {},
   "source": [
    "### Resources used:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9042de83-cf0d-41c1-b9b6-6e53a25bf0fa",
   "metadata": {},
   "source": [
    "- https://stackoverflow.com/questions/51325601/how-to-stop-my-pandas-data-table-from-being-truncated-when-printed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loans_capstone",
   "language": "python",
   "name": "loans_capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
