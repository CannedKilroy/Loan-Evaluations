{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "037725ff-a7c1-4cef-b3c2-c79ee123a8e1",
   "metadata": {},
   "source": [
    "# Advanced models: SVM, DecisionTrees, Randomforest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbeb84b-a0aa-483b-8e24-52b336745a00",
   "metadata": {},
   "source": [
    "## Date: Nov 21, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33183941-9962-49fb-b21e-b1666c558f67",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663de5f7-2f6d-446d-ac11-e643fca27f07",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7b2419-222b-49b7-84b0-6bbec652afa7",
   "metadata": {},
   "source": [
    "willl use gridearch for hyperparamter optimization   \n",
    "kfolds cross validiaton\n",
    "does not have odds ratio explainability\n",
    "Note:\n",
    "- SVM's are very slow, especially non linear ones. Running the models may take over an hour. The sklearnex patch_sklearn() has helped tremendously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f8a524-9c36-4a80-a212-b117449a0b06",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043c982c-c0b5-4afc-86b8-e064139625bb",
   "metadata": {},
   "source": [
    "### Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6707042-0059-4602-a55b-072880c0854a",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)\n",
    "   - [Table of Contents](#Table-of-contents)\n",
    "   - [Import Librarys](#Import-Librarys)\n",
    "   - [Data Dictionary](#Data-Dictionary)\n",
    "   - [Load the data](#Load-the-data)\n",
    "3. [SVM Model](#SVM-Model)\n",
    "   - [Assumptions](#Assumptions)\n",
    "   - [PreProcessing](#PreProcessing)\n",
    "   - [1st Iteration](#1st-Iteration)\n",
    "   - [Evaluation](#Evaluation)\n",
    "8. [Conclusion](#Conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c61337-6378-4d03-911d-bb91287b1309",
   "metadata": {},
   "source": [
    "### Import Librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eb7e11-9724-442d-ad58-b171fd62bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import make_column_selector\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from pathlib import Path\n",
    "from sklearnex import patch_sklearn \n",
    "\n",
    "\n",
    "from helpers import display_corr_heatmap, data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74452ebf-f811-4fa0-8956-31100f6c3edc",
   "metadata": {},
   "source": [
    "Sklearnex will be used to speed up the sklearn library. If any bugs occur, simply comment out the code below/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef96c5ed-7b11-4dfa-b3a0-c796c7879f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accelerate sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f48ef04-aeb2-4ba8-8002-0b8f8c12c878",
   "metadata": {},
   "source": [
    "### Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b68fb-9a10-4335-a9f0-0f1928e10a77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e8122c-e9fe-43e3-a2a1-8c5e03243910",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917ffc47-0d1c-4776-a406-c2c1155b4a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the relative path to the file\n",
    "parquet_file_path = Path('../Data/Lending_club/model_cleaned')\n",
    "\n",
    "try:\n",
    "    # Read the parquet file\n",
    "    loans_df = pd.read_parquet(parquet_file_path)\n",
    "except FileNotFoundError as e:\n",
    "    print(e.args[1])\n",
    "    print('Check file location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b7976b-d3a7-4de4-b14c-af342b960825",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12504c05-b658-4150-a241-8187ce6512c5",
   "metadata": {},
   "source": [
    "### SVM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde2087-03fc-4c56-af62-17b6fa2089e0",
   "metadata": {},
   "source": [
    "Similarly to log reg, SVM creates linear boundaries. However, SVM aims to to maximize the decision boundary distance between classes, while minimizing misclassified data points. This increased buffer created between classes should better capture the extra variance when being evaluated on the test set. \n",
    "\n",
    "As SVM is a linear model (for the linear kernal), it is assumed that the data is linearly separable, to some degree atleast, and that there is no colinearity or multicolinearity. These linear considerations are less important for the non linear kernals. IID is also assumed.\n",
    "The hyper parameters to be used will be used:\n",
    "\n",
    "- C value: The C hyperparameter tells the SVM optimization how much to avoid misclassifying each training example. For very large C values, the SVM will fit the training data very closely and tends to find a very small margin in favor of making fewer mistakes. \n",
    "For very small C values, the SVM will be allowed to pick a wider margin by misclassifying more point. Since accuracy is preferred for this project, a higher C value is going to be preferred, without overfitting.\n",
    "- Scaler: As SVM is distance based, it will benefit from distance scaling. \n",
    "- Kernal: A linear kernal will be tried, but different kernals to better handle non linearity can be used.\n",
    "\n",
    "More information on SVM's:  \n",
    "https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f51a504-87ed-4782-bfd5-91e8fdb6ad97",
   "metadata": {},
   "source": [
    "## Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e078e061-a524-4a95-aabd-6e26e0c20956",
   "metadata": {},
   "source": [
    "### Colinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299daee2-89fe-4b2c-9fa6-759fab7a0164",
   "metadata": {},
   "source": [
    "Plot a correlation heatmap for the remaining features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e115faaf-3f27-4480-bb48-1e981984adc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_corr_heatmap(loans_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c345de34-e45c-4942-bb0e-f76643725763",
   "metadata": {},
   "source": [
    "Check for multicollinearity and collineartiy before splitting the data or encoding categorical variables. First check for multicollinearity using Variance Inflation Factor (VIF). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3620d25c-edae-42f1-8daf-bafd07757b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = loans_df.select_dtypes(include=[np.number])\n",
    "\n",
    "#define a vif threshold\n",
    "vif_cutoff = 10\n",
    "\n",
    "#create a dataframe to hold the vif scores for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = numeric_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db58203a-09f0-482f-9bc4-a530572a8f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#calculate the vif. This may take a few minutes\n",
    "print('Running vif calculations ...')\n",
    "vif_data['VIF'] = [variance_inflation_factor(numeric_df.values, i) for i in range(len(numeric_df.columns))]\n",
    "print('Finished vif calculations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159b8c8c-7ebb-471d-8c23-258ab2c605ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sort vif's in descending order\n",
    "vif_data.sort_values(by=['VIF'], ascending=False)\n",
    "\n",
    "#filter columns with a vif greater than the cutoff and place in a list\n",
    "high_vif_columns = vif_data[vif_data['VIF'] > vif_cutoff]['feature'].tolist()\n",
    "display(high_vif_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfd0d80-f124-41a5-9abb-962bb9ce676a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop features with high VIF\n",
    "# https://easystats.github.io/performance/reference/check_collinearity.html#:~:text=Interpretation%20of%20the%20Variance%20Inflation%20Factor&text=A%20VIF%20less%20than%205,model%20predictors%20(James%20et%20al.\n",
    "filtered_high_vif_columns = [feature for feature in high_vif_columns if feature not in ['loan_amnt', 'term', 'int_rate']]\n",
    "\n",
    "loans_df.drop(columns = filtered_high_vif_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ce67ff-5684-4401-8806-1bdd613a060b",
   "metadata": {},
   "source": [
    "The remaining features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723eea7c-9d12-4348-a787-23fca2248767",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_df.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cc92bd-a349-4a6f-8083-94f8a446ca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_corr_heatmap(loans_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19620efc-eb46-42bf-89da-6ef8a1d78455",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa77465-4942-43c6-beaf-67de43b0213f",
   "metadata": {},
   "source": [
    "***Train test split***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81766bf-0a91-49e8-a028-200721602293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X = loans_df.drop(columns=['loan_status'], inplace=False)\n",
    "y = loans_df['loan_status']\n",
    "\n",
    "# Split into train and test sets. Stratify to ensure any inbalance is preserved as in the original data. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=11, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5d2ee1-3836-4f96-adc4-a39017627e76",
   "metadata": {},
   "source": [
    "***Data Inbalance***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f055e36-4be7-4415-9d51-fcc387e1ff04",
   "metadata": {},
   "source": [
    "As shown in the log reg and EDA notebooks, the data is inbalanced. For svm's class weights can be used to combat this, however, we have more than enough data to properly balance the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc0c6d2-8074-4340-8342-3c4ed9b3e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of class 1 examples before:', X_train[y_train == 1].shape[0])\n",
    "\n",
    "# Downsample majority class\n",
    "X_downsampled, y_downsampled  = resample(X_train[y_train == 1],\n",
    "                                   y_train[y_train == 1],\n",
    "                                   replace=False,\n",
    "                                   n_samples=X_train[y_train == 0].shape[0],\n",
    "                                   random_state=1)\n",
    "\n",
    "print('\\nNumber of class 1 examples after:', X_downsampled.shape[0])\n",
    "\n",
    "# Combine the downsampled successful loans with the failed loans. Will keep as a df since changing to \n",
    "X_train_bal = pd.concat([X_train[y_train == 0], X_downsampled])\n",
    "y_train_bal = np.hstack((y_train[y_train == 0], y_downsampled))\n",
    "\n",
    "print(\"New X_train shape: \", X_train_bal.shape)\n",
    "print(\"New y_train shape: \", y_train_bal.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788c2dc3-2eba-44d0-a391-b04dc8f1ecfe",
   "metadata": {},
   "source": [
    "***Inspect Categorical Features***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c15182-1bb6-446b-aa8e-ae3c1b7fe034",
   "metadata": {},
   "source": [
    "Similarly to the log_reg model, categorical features will be encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83efe6ed-e1f4-4b7c-a9c8-3ee6ac3316c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = X_train_bal.select_dtypes('object').columns.tolist()\n",
    "display(categorical_columns)\n",
    "categorical_columns.remove('verification_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3968be02-eaaa-498c-bc82-024a8b3d96e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate onehot encoder\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "#instantiate ordinal encoder\n",
    "ordinal_transformer = OrdinalEncoder(categories=[['Not Verified', 'Source Verified', 'Verified']])\n",
    "\n",
    "#combine into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, ['home_ownership', 'verification_status', 'purpose', 'application_type']),\n",
    "        ('ord', ordinal_transformer, ['verification_status'])],\n",
    "    remainder='passthrough',\n",
    "    n_jobs=2\n",
    ")\n",
    "\n",
    "#fit to the train set\n",
    "preprocessor.fit(X_train_bal)\n",
    "\n",
    "#transform the train and test sets\n",
    "X_train_transformed = preprocessor.transform(X_train_bal)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Shape of train transformed: \", X_train_transformed.shape)\n",
    "print(\"Shape of test transformed: \", X_test_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d2dace-bf15-49d7-8f1d-c6c01c115a9a",
   "metadata": {},
   "source": [
    "### 1st Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd852645-93df-43cf-9ace-814f208c988a",
   "metadata": {},
   "source": [
    "For the first iteration, the C value will be varied. This is the most important hyperparameter for SVM's. Manually iterating will give us a target range when we do the gridsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae5ac06-9264-4f10-8e09-7fc2280baaf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_transformed)\n",
    "X_test_scaled = scaler.transform(X_test_transformed)\n",
    "\n",
    "# Define a range of C values\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# Lists to store results\n",
    "C_values_used = []\n",
    "f1_scores = []\n",
    "precision_scores_class_1 = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "\n",
    "# Loop through each C value\n",
    "for C in C_values:\n",
    "\n",
    "    print()\n",
    "    print(f'Starting the loop for C value: {C}')\n",
    "    \n",
    "    # Fit an SVM model\n",
    "    print('Fitting the model')\n",
    "    model = SVC(C=C, kernel='linear', max_iter=300, verbose=1, cache_size=800, random_state=1)\n",
    "    model.fit(X_train_scaled, y_train_bal)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    print('Evaluating')\n",
    "    y_pred_train = model.predict(X_train_scaled)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    #get the specific scores out for class 1\n",
    "    f1 = f1_score(y_test, y_pred, average='binary', pos_label=1)\n",
    "    precision = precision_score(y_test, y_pred, pos_label=1)\n",
    "    \n",
    "   # Calculate accuracies\n",
    "    train_accuracy = accuracy_score(y_train_bal, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    # Store results\n",
    "    C_values_used.append(C)\n",
    "    f1_scores.append(f1)\n",
    "    precision_scores_class_1.append(precision)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"C value: {C}\")\n",
    "    print(f\"F1 Score for class 1: {f1}\")\n",
    "    print(f\"Precision for class 1: {precision}\")\n",
    "    print()\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print(\"---------------------------------------------------\")\n",
    "\n",
    "# Print summary of results\n",
    "print(\"Finished training and evaluation.\")\n",
    "print(\"C values used:\", C_values_used)\n",
    "print(\"F1 Scores for class 1:\", f1_scores)\n",
    "print(\"Precision for class 1:\", precision_scores_class_1)\n",
    "print(\"Train Accuracies:\", train_accuracies)\n",
    "print(\"Test Accuracies:\", test_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b85718f-078e-4b47-83b8-3a50a1b0f090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94114ba3-c2b4-4545-8c21-75dc8f04e268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8907648d-7db1-4f3a-8336-95735a3cecb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e19340-6f2a-424b-a29a-68de9002e373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... (previous code) ...\n",
    "\n",
    "# Plotting the F1 Scores and Precision for class 1\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot F1 scores for class 1\n",
    "plt.plot(C_values_used, f1_scores, marker='o', label='F1 Score for class 1')\n",
    "plt.plot(C_values_used, precision_scores_class_1, marker='x', linestyle='--', label='Precision for class 1')\n",
    "plt.plot(C_values_used, train_accuracies, marker='*', linestyle='-', label='Train accuracy')\n",
    "plt.plot(C_values_used, test_accuracies, marker='^', linestyle='-', label='Test accuracy')\n",
    "\n",
    "\n",
    "\n",
    "plt.xscale('log')  # Since C values vary in orders of magnitude\n",
    "plt.xlabel('C value')\n",
    "plt.ylabel('Score')\n",
    "plt.title('SVM Model Performance for Different C Values')\n",
    "plt.legend()  # Add a legend to distinguish the lines\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb319f4-adcc-422b-bace-0234df1560ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(C_values_used, f1_scores, marker='o')\n",
    "plt.xscale('log')  # Since C values vary in orders of magnitude\n",
    "plt.xlabel('C value')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('SVM Model Performance for Different C Values')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6172b025-ecf7-4c0b-b755-579552f75bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a0326-75e0-4f4e-9b27-e67ce05266e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740b4b49-d43f-4140-aafd-7f1c7252d4db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61d58f-a1ef-402d-acac-21333e6efffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_transformed)\n",
    "X_test_scaled = scaler.transform(X_test_transformed)\n",
    "\n",
    "# Define a range of C values\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# Loop through each C value\n",
    "for C in C_values:\n",
    "    print(f'Starting the loop for C value: {C}')\n",
    "    \n",
    "    # Fit an SVM model\n",
    "    print('Fitting the model')\n",
    "    model = SVC(C=C, kernel='linear', probability=True, verbose=1, random_state=1)\n",
    "    model.fit(X_train_scaled, y_train_bal)\n",
    "\n",
    "    print('Evaluating')\n",
    "    # Predict and evaluate\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"C value: {C}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print(\"---------------------------------------------------\")\n",
    "\n",
    "    # Calculate train accuracy\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    train_accuracy = accuracy_score(y_train_bal, y_train_pred)\n",
    "    print(f\"Train Accuracy: {train_accuracy}\")\n",
    "\n",
    "    # Calculate AUC\n",
    "    probs = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    roc_auc = roc_auc_score(y_test, probs)\n",
    "    print(f'Area Under Curve (AUC): {roc_auc:.2f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9ddfb7-3459-42e7-91d0-f4461591bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "# Define a range of C values\n",
    "param_range = np.logspace(-4, 4, 20)\n",
    "\n",
    "# Calculate validation curve\n",
    "train_scores, test_scores = validation_curve(\n",
    "    LogisticRegression(),\n",
    "    X_train_transformed, y_train_bal, \n",
    "    param_name=\"C\", param_range=param_range,\n",
    "    cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "\n",
    "# Calculate mean and standard deviation for training set scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "\n",
    "# Calculate mean and standard deviation for test set scores\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "# Plot mean accuracy scores for training and test sets\n",
    "plt.plot(param_range, train_mean, label=\"Training score\", color=\"black\")\n",
    "plt.plot(param_range, test_mean, label=\"Cross-validation score\", color=\"dimgrey\")\n",
    "\n",
    "# Create plot\n",
    "plt.title(\"Validation Curve With Logistic Regression\")\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.tight_layout()\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f8f729-5be7-43e2-a3f2-396887b2c221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fec4949-16cf-4a7e-8422-426a1f55cc23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6be3cb-0d63-4f4c-b27c-7a742c4afea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc12c4c-8bbc-4f63-b8b8-52091d88a5f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define different C values to test\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "# Initialize the scaler just once, outside the loop\n",
    "#scaler = RobustScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply scaler to train data\n",
    "X_train_scaled = scaler.fit_transform(X_train_transformed)\n",
    "\n",
    "# Apply scaler to test data\n",
    "X_test_scaled = scaler.transform(X_test_transformed)\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "# Loop over the C values\n",
    "for C in C_values:\n",
    "    \n",
    "    # Create and fit the model\n",
    "    model = SVC(C=C, verbose=3, random_state=1)\n",
    "    model.fit(X_train_scaled, y_train_bal)\n",
    "\n",
    "    # Calculate and store the scores\n",
    "    train_score = model.score(X_train, y_train)\n",
    "    test_score = model.score(X_test, y_test)\n",
    "\n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "\n",
    "# Plotting the scores\n",
    "plt.plot(C_values, train_scores, label='Train')\n",
    "plt.plot(C_values, test_scores, label='Test')\n",
    "plt.xscale('log')  # Since C values vary exponentially\n",
    "plt.xlabel('C value')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs. C value for Logistic Regression')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Loop through each C value\n",
    "for C in C_values:\n",
    "    \n",
    "    print(f'Starting the loop for C value: {C}')\n",
    "    \n",
    "    # Fit an SVM model\n",
    "    print('Fitting the model')\n",
    "    model = SVC(C=C, verbose=1, random_state=1, )\n",
    "    model.fit(X_train_scaled, y_train_bal)\n",
    "\n",
    "    print('Evaluating')\n",
    "    # Predict and evaluate\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"C value: {C}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print(\"---------------------------------------------------\")\n",
    "\n",
    "    # If you want to track train accuracy as well, you'll need to predict on the train set\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    train_accuracy = accuracy_score(y_train_bal, y_train_pred)\n",
    "    print(f\"Train Accuracy: {train_accuracy}\")\n",
    "\n",
    "    #AUC\n",
    "    #calculate the probability scores\n",
    "    #probs = model.predict_proba(X_test_scaled)\n",
    "    #roc_auc = roc_auc_score(y_test, probs)\n",
    "    #print(f'Area under curve (AUC):{roc_auc}')\n",
    "    #print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5701bf-ac9f-43b8-a632-711a8c2dc67e",
   "metadata": {},
   "source": [
    "For the first iteration, we will manually vary the scaler. Since SVM is distance based, it is sensitive to how the data is scaled and shifted. The standard scaler is most common and shifts the data so that the mean is 0 and standard deviation (STD) of 1. However, it assumes the data follows a normal distribution. The minmax scaler scales the data such that is lands within the range [0,1]. Finally, the robust scaler works essentially the same as the standard scaler except it uses the median and IQR opposed to mean and STD. Since robust scaler uses the median opposed to mean, it handles outliers and skewness better than the standard scaler.\n",
    "\n",
    "More information on the different scalers can be found here:  \n",
    "https://medium.com/@onersarpnalcin/standardscaler-vs-minmaxscaler-vs-robustscaler-which-one-to-use-for-your-next-ml-project-ae5b44f571b9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9b32ad-1bcf-4b3b-9f44-760cf8c95e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#instaniate the scalers\n",
    "standard_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# List of scalers\n",
    "scalers = [standard_scaler, minmax_scaler, robust_scaler]\n",
    "scaler_names = ['StandardScaler', 'MinMaxScaler', 'RobustScaler']\n",
    "\n",
    "# Loop through each scaler\n",
    "for scaler, name in zip(scalers, scaler_names):\n",
    "    \n",
    "    print(f'Starting the loop for scaler: {name}')\n",
    "    \n",
    "    # Apply scaler to train and test data\n",
    "    X_train_scaled = scaler.fit_transform(X_train_transformed)\n",
    "    X_test_scaled = scaler.transform(X_test_transformed)\n",
    "    print('Applied the scaler')\n",
    "\n",
    "    # Fit an SVM model\n",
    "    print('Fitting the model')\n",
    "    #model = LinearSVC(max_iter=10000, verbose=1)\n",
    "    model = SVC(verbose=1, random_state=1)\n",
    "    model.fit(X_train_scaled, y_train_bal)\n",
    "\n",
    "    print('Evaluating')\n",
    "    # Predict and evaluate\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    #print the results\n",
    "    print(f\"Scaler: {name}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print(\"---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b00ebe-fec2-42bf-9d69-5d1fc198a8b6",
   "metadata": {},
   "source": [
    "We can plot the scores over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f993a7-a36a-4139-b134-b6d23c2dbc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the feature weights out. \n",
    "feature_weights = pd.DataFrame({\n",
    "    'Feature': preprocessor.get_feature_names_out(),\n",
    "    'Coefficient': model.coef_[0]\n",
    "})\n",
    "\n",
    "# Sort the features by the absolute value of their coefficient\n",
    "feature_weights = feature_weights.sort_values(by='Coefficient', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bb53d1-ebdc-47b4-88ee-c2e757b78dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the feature weights\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.barh(feature_weights['Feature'], feature_weights['Coefficient'], color='lightblue')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dca4ae-be37-4466-ae4c-564c041adef7",
   "metadata": {},
   "source": [
    "Hyperparameters to use:\n",
    "Since we dont know what parameters to use exactly, RandomizedSearchCV can be used. It is essentially a gridsearch sv but only takes a small sample. This allows us to gauge what the range for our hyperparamters should be. \n",
    "In general different kernels work well for different tasks, and in the absence of prior knowledge sometimes the best option is to try out various kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900d0268-1460-4863-988b-36e848d37e2f",
   "metadata": {},
   "source": [
    "For the first iteration, we will manually vary the scaler. Since SVM is distance based, it is sensitive to how the data is scaled and shifted. The standard scaler is most common and shifts the data so that the mean is 0 and standard deviation (STD) of 1. However, it assumes the data follows a normal distribution. The minmax scaler scales the data such that is lands within the range [0,1]. Finally, the robust scaler works essentially the same as the standard scaler except it uses the median and IQR opposed to mean and STD. Since robust scaler uses the median opposed to mean, it handles outliers and skewness better than the standard scaler.\n",
    "\n",
    "More information on the different scalers can be found here:  \n",
    "https://medium.com/@onersarpnalcin/standardscaler-vs-minmaxscaler-vs-robustscaler-which-one-to-use-for-your-next-ml-project-ae5b44f571b9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2639b7-0eb1-4cd9-a7ef-a7b3f5b666e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loans_capstone",
   "language": "python",
   "name": "loans_capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
