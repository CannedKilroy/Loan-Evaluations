{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7737e2f6-ee61-4939-b1e9-4d86244412cc",
   "metadata": {},
   "source": [
    "# Base Model: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb8d57e-f8e2-438f-ac20-e55f148bb495",
   "metadata": {},
   "source": [
    "## Date: Nov 9, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256c83e4-ba8b-41a0-a026-0ebaeec4d1b3",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98074a2c-ee81-473a-a32e-34eb6dc688d8",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae00c66-f311-4423-ad64-b7943e0178cf",
   "metadata": {},
   "source": [
    "In this notebook, a baseline classification model will be established using Logistic Regression. Log reg is a good baseline as it is one of the simplest classification models, offers high explainability, and is computationally light compared to its counterparts. Specifically, it offers greater explainability via odds ratio than its more optimized counterpart SVM. After the data is read in, basic assumptions are established and checked. Any features that show high colinearity and multicollinearity are removed. Then 3 iterations of the log reg will are run.\n",
    "1. Unbalanced unscaled dataset. This allows us to evaluate how balancing the dataset affects the model's performance\n",
    "2. Balanced scaled dataset. This is the first baseline model to be used\n",
    "3. Optimized model. A optimized model will be used by varying the solver, iterations, regulatization to achieve the best baseline log reg model.\n",
    "\n",
    "The optimizations will be done manually as to demonstrate the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf6673d-f070-4f0b-96bd-706e29346538",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8497c5-d4a9-4dea-b86e-238a39652efb",
   "metadata": {},
   "source": [
    "### Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7801100c-f90c-4284-b341-38ad75aa8570",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)\n",
    "   - [Table of Contents](#Table-of-contents)\n",
    "   - [Import Librarys](#Import-Librarys)\n",
    "   - [Data Dictionary](#Data-Dictionary)\n",
    "   - [Define Functions](#Define-Functions)\n",
    "   - [Load the data](#Load-the-data)\n",
    "3. [Logistic Regression Model](#Logistic-Regression-Model)\n",
    "   - [Assumptions](#Assumptions)\n",
    "   - [PreProcessing](#PreProcessing)\n",
    "   - [Modelling](#Modelling)\n",
    "   - [Evaluation](#Evaluation)\n",
    "8. [Conclusion](#Conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8125e564-2b37-4bdb-a886-123ed781112a",
   "metadata": {},
   "source": [
    "### Import Librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cbeb4b-1a60-4ed0-860d-f4b52aa2d41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import make_column_selector\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25946e1f-a975-4d39-a161-bd37d2646a21",
   "metadata": {},
   "source": [
    "### Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6636a5f3-646d-4698-89d9-cb0c4c780f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ff308-4904-4076-a245-40e57fcd7c29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pathlib is used to ensure compatibility across operating systems\n",
    "try:\n",
    "    data_destination = Path('../Data/Lending_club/Lending Club Data Dictionary Approved.csv')\n",
    "    dict_df = pd.read_csv(data_destination, encoding='ISO-8859-1')\n",
    "    display(dict_df.iloc[:,0:2])\n",
    "except FileNotFoundError as e:\n",
    "    print(e.args[1])\n",
    "    print('Check file location')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249e4929-f85e-492b-a8ba-b9a4e5772d02",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e07d49-3123-43f8-92e6-d650d28887d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the relative path to the file\n",
    "parquet_file_path = Path('../Data/Lending_club/model_cleaned')\n",
    "\n",
    "try:\n",
    "    # Read the parquet file\n",
    "    loans_df = pd.read_parquet(parquet_file_path)\n",
    "except FileNotFoundError as e:\n",
    "    print(e.args[1])\n",
    "    print('Check file location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a0c361-0ecf-4d29-b758-8f938f49c20e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loans_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bc45ce-c611-4dd2-93db-d3090f11b343",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c95760f-a9ab-428a-bb0f-a4b864a7e813",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731264c4-c62d-43a6-bd94-5deb112bf41b",
   "metadata": {},
   "source": [
    "### Assumptions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b262f00-2669-4ea2-b17b-058e6083bc9d",
   "metadata": {},
   "source": [
    "Before we can start modeling, some base assumptions must be met in order to use a log reg model.   \n",
    "These include:  \n",
    "* **Binary Outcome:** The dependent variable is a binary. This is met as loan status has been encoded as 1 and 0\n",
    "* **Independence:** It is reasonable to assume loans are independent. Without identifiable information, there is not way of knowing from the dataset whether a borrower has applied for multiple loans as the member_id data has been removed by lendingclub.\n",
    "* **No collinearity / multicollinearity.** This will be checked\n",
    "* **Sufficiently Large sample size:** This is met"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d3d311-cddd-46da-b6a3-3a0e9561c855",
   "metadata": {},
   "source": [
    "### Colinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6400cb8-463f-4b7d-8f9e-ccf79d6ddb60",
   "metadata": {},
   "source": [
    "Plot a correlation heatmap for the remaining features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b013db7-d686-4335-af78-47b99b248c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the numeric columns for the correlation matrix\n",
    "numeric_df = loans_df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr = numeric_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(corr, mask=mask, cmap='coolwarm', vmax=1, vmin=-1, center=0,\n",
    "            square=True, linewidths=.5, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce377bcb-c34b-4cfe-b1d1-ec0dfb2c988c",
   "metadata": {},
   "source": [
    "### Collinearity / Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea894f-419d-464e-84ed-2e35e679cbd2",
   "metadata": {},
   "source": [
    "We will check for multicollinearity and collineartiy before we split the data or encode categorical variables. We will first check for multicollinearity using Variance Inflation Factor (VIF). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030684c4-3cb8-407c-9b30-338c14033a3b",
   "metadata": {},
   "source": [
    "Create a dataframe with the vif values for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a1a75b-aee5-4dc3-ad6c-70ae2713cc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe to hold the vif scores for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = numeric_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c85f7f5-9dc3-4970-8c46-ca765670cec6",
   "metadata": {},
   "source": [
    "Calculate the vif scores for each feature and place in the dataframe. This may take a few moments as it is running a linear regression between each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50b4271-3a5a-45a8-94f9-19bab920a590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define a vif threshold\n",
    "vif_cutoff = 10\n",
    "\n",
    "#calculate the vif. This may take a few minutes\n",
    "print('Running vif calculations')\n",
    "vif_data['VIF'] = [variance_inflation_factor(numeric_df.values, i) for i in range(len(numeric_df.columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd81e97f-e3d0-460d-aadb-0b31226167c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vif_data.sort_values(by=['VIF'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66778510-20b9-43f6-9def-b2d67e9f920f",
   "metadata": {},
   "source": [
    "Create a list of the columns with a vif greater than the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03d5236-a6fa-4f08-b101-40c303fcc0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_vif_columns = vif_data[vif_data['VIF'] > vif_cutoff]['feature'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616ea539-1327-4573-9628-c3af467756c1",
   "metadata": {},
   "source": [
    "Before we drop the features with high vif, we will inspect them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2d9eda-d873-4804-abf9-7e9deddfd56a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(high_vif_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2cf50d-a133-4542-87b7-1e84789403eb",
   "metadata": {},
   "source": [
    "We will leave `loan_amnt`, `term`, `int_rate`, as these are key features of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0788b5be-564e-4b8b-8232-8c82c79397ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop features with high VIF\n",
    "# https://easystats.github.io/performance/reference/check_collinearity.html#:~:text=Interpretation%20of%20the%20Variance%20Inflation%20Factor&text=A%20VIF%20less%20than%205,model%20predictors%20(James%20et%20al.\n",
    "filtered_high_vif_columns = [feature for feature in high_vif_columns if feature not in ['loan_amnt', 'term', 'int_rate']]\n",
    "\n",
    "loans_df.drop(columns = filtered_high_vif_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3e7a22-8da7-4e58-afb3-89ea304a3150",
   "metadata": {},
   "source": [
    "The remaining features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3bdfff-8e2a-41e3-ac42-34cc4b7576e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_df.head(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780db858-d49e-4d4f-9704-ac3e9102c8a5",
   "metadata": {},
   "source": [
    "***Collinearity***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1278d4f2-f029-4cbe-8b79-0df3ee80241a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062ac3d3-002a-4608-a34a-3ab4ce9551f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the numeric columns for the correlation matrix\n",
    "numeric_df = loans_df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr = numeric_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(corr, mask=mask, cmap='coolwarm', vmax=1, vmin=-1, center=0,\n",
    "            square=True, linewidths=.5, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42b7617-8fca-424e-89ed-f5547428ffd3",
   "metadata": {},
   "source": [
    "There are still some high correlations between variables. There are no major features highly correlated with the target variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a6577-42b1-430d-9d2e-b87146fd6692",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "All the assumptions have now beeen met"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a91e07-d156-424d-8e5f-c6149a3e34b6",
   "metadata": {},
   "source": [
    "### PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee49f3d-e7a2-4de0-b7bd-b5cde14cd2e5",
   "metadata": {},
   "source": [
    "For the first iteration, no scaling or resampling will be done. Only encoding for the categorical variables. For the second iteration, a standard scaler is, and the data inbalance is addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f379563b-3fb6-4de2-b8dc-a86e594d04b1",
   "metadata": {},
   "source": [
    "***Train Test Split***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c767a32e-3e79-4bdc-82b7-2dc6f3434252",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X = loans_df.drop(columns=['loan_status'], inplace=False)\n",
    "y = loans_df['loan_status']\n",
    "\n",
    "# Split into train and test sets. Stratify to ensure any inbalance is preserved as in the original data. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=11, stratify=y)\n",
    "\n",
    "# Split into train and test sets except unbalanced\n",
    "unbal_X_train, unbal_X_test, unbal_y_train, unbal_y_test = train_test_split(X, y, test_size=0.3, random_state=11, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50740bc5-c66e-42ee-9169-d3ae76fc6ee6",
   "metadata": {},
   "source": [
    "***Data Inbalance***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e7c66c-887e-402a-af9a-4c071707f50b",
   "metadata": {},
   "source": [
    "As shown in EDA, there is a large inbalance between the number of successful loans (class 1) and failed loans (class 0), approximately 80/20. Since the dataset is sufficiently large, it is acceptable to downsample the instances of class 1 to equal class 0. Balancing the dataset reduces the risk of any bias introduced to a single class simply due to its frequency in the dataset.  \n",
    "\n",
    "This also has the added advantage of giving a more manageable dataset size. However, if computation power is not an issue, then more failed loans could be sampled from the original dataset, and / or synthetic data created for the minority class using SMOTE.  \n",
    "More information can be found here:  \n",
    "https://towardsdatascience.com/smote-fdce2f605729"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f238a59d-9a4e-434b-8a60-4b6e0016b21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of class 1 examples before:', X_train[y_train == 1].shape[0])\n",
    "\n",
    "# Downsample majority class\n",
    "X_downsampled, y_downsampled  = resample(X_train[y_train == 1],\n",
    "                                   y_train[y_train == 1],\n",
    "                                   replace=False,\n",
    "                                   n_samples=X_train[y_train == 0].shape[0],\n",
    "                                   random_state=1)\n",
    "\n",
    "print('\\nNumber of class 1 examples after:', X_downsampled.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed11a6f-9e27-46f2-818b-adf17e0ef779",
   "metadata": {},
   "source": [
    "Can now combine with the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2389e754-7959-4c90-b399-89494fe06bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the downsampled successful loans with the failed loans. Will keep as a df since changing to \n",
    "X_train_bal = pd.concat([X_train[y_train == 0], X_downsampled])\n",
    "y_train_bal = np.hstack((y_train[y_train == 0], y_downsampled))\n",
    "\n",
    "print(\"New X_train shape: \", X_train_bal.shape)\n",
    "print(\"New y_train shape: \", y_train_bal.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f131f6d8-c167-46a1-9b1e-74f63ef27a25",
   "metadata": {},
   "source": [
    "***Inspect Categorical Features***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1139764-8708-4399-9409-95b5e303ab98",
   "metadata": {},
   "source": [
    "Inspect whether the categorical features are ordinal or nominal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82adf0b5-4424-4ee9-a4a6-c0c364d85033",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = X_train_bal.select_dtypes('object').columns.tolist()\n",
    "display(categorical_columns)\n",
    "categorical_columns.remove('verification_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5579c0db-f0f0-45d0-8348-0e4c2c7a08ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bal['verification_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff8acb7-9d0d-45a3-87db-37275b0cf594",
   "metadata": {},
   "source": [
    "The feature `verification_status` will be **ordinal encoded** since loan applications with verified information should be weighted higher than those with unverified info. The other categorical features can be onehot encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d110f555-64a8-403b-8df0-60cce98f9efd",
   "metadata": {},
   "source": [
    "***Column Transformation for 1st iteration***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3185c022-b585-4675-9a77-1042f48cf51e",
   "metadata": {},
   "source": [
    "Just encode the categorical variables as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeef741-d738-4e8a-9948-ce04c9db590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate onehot encoder\n",
    "unbal_categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "#instantiate ordinal encoder\n",
    "unbal_ordinal_transformer = OrdinalEncoder(categories=[['Not Verified', 'Source Verified', 'Verified']])\n",
    "\n",
    "#instantiate the column transformer\n",
    "unbal_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', unbal_categorical_transformer, ['home_ownership', 'purpose', 'application_type']),\n",
    "        ('ord', unbal_ordinal_transformer, ['verification_status']),\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    n_jobs=2 #use 2 cpu cores for greater speed\n",
    ")\n",
    "\n",
    "#fit to the train set\n",
    "unbal_preprocessor.fit(unbal_X_train)\n",
    "\n",
    "#transform the train and test sets\n",
    "unbal_X_train_transformed = unbal_preprocessor.transform(unbal_X_train)\n",
    "unbal_X_test_transformed = unbal_preprocessor.transform(unbal_X_test)\n",
    "\n",
    "print(\"Shape of train transformed: \", unbal_X_train_transformed.shape)\n",
    "print(\"Shape of test transformed: \",  unbal_X_test_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7779894b-ebd8-4bf7-b713-d73ecb891bf1",
   "metadata": {},
   "source": [
    "***Column Transformation for 2nd Iteration***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216f5e8-2962-4447-9405-ab9789f42ade",
   "metadata": {},
   "source": [
    "For the second iteration, a standard scaler is fit as well. Although log reg is not a distance based model, it can aid in model performance by reducing the size of the parameter space, allowing the model to converge more easily.  \n",
    "More information can be found here:  \n",
    "https://forecastegy.com/posts/does-logistic-regression-require-feature-scaling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cac96be-04ff-4479-a4fd-4d1adb44ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate onehot encoder\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "#instantiate ordinal encoder\n",
    "ordinal_transformer = OrdinalEncoder(categories=[['Not Verified', 'Source Verified', 'Verified']])\n",
    "\n",
    "#combine into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, ['home_ownership', 'purpose', 'application_type']),\n",
    "        ('ord', ordinal_transformer, ['verification_status']),\n",
    "        ('num', StandardScaler(), make_column_selector(dtype_include=['int64','int32','float64','float32']))\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    n_jobs=2\n",
    ")\n",
    "\n",
    "#fit to the train set\n",
    "preprocessor.fit(X_train_bal)\n",
    "\n",
    "#transform the train and test sets\n",
    "X_train_transformed = preprocessor.transform(X_train_bal)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Shape of train transformed: \", X_train_transformed.shape)\n",
    "print(\"Shape of test transformed: \", X_test_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff635be9-272a-42de-9f19-2d39889574d2",
   "metadata": {},
   "source": [
    "***Run the model 1st iteration***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce2f0b0-db80-4773-916d-933d83d39bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing and training the logistic regression model\n",
    "unbal_log_reg = LogisticRegression(random_state=1,\n",
    "                             solver='lbfgs', \n",
    "                             max_iter=4000, \n",
    "                             verbose=2, #output while the model runs\n",
    "                             n_jobs=2) #use 2 cpu cores\n",
    "                             #class_weight='balanced') #weight the class to counter more frequent class\n",
    "\n",
    "unbal_log_reg.fit(unbal_X_train_transformed, unbal_y_train)\n",
    "\n",
    "# Making predictions on the test data using the trained model\n",
    "unbal_y_pred = unbal_log_reg.predict(unbal_X_test_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876c0af6-212d-4663-8a63-0a7271bb2eaa",
   "metadata": {},
   "source": [
    "***Score the 1st iteration model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7072ba48-77b8-4481-84b9-5fe99b8104a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring the model on both train and test data\n",
    "unbal_train_score = unbal_log_reg.score(unbal_X_train_transformed, unbal_y_train)\n",
    "unbal_test_score = unbal_log_reg.score(unbal_X_test_transformed, unbal_y_test)\n",
    "print(f'Score on train: {unbal_train_score}')\n",
    "print(f'Score on test: {unbal_test_score}')\n",
    "\n",
    "# Evaluating the model with confusion matrix and a classification report\n",
    "conf_matrix = confusion_matrix(unbal_y_test, unbal_y_pred)\n",
    "class_report = classification_report(unbal_y_test, unbal_y_pred)\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    unbal_log_reg, \n",
    "    unbal_X_test_transformed, \n",
    "    unbal_y_test, \n",
    "    cmap='Blues', \n",
    "    display_labels=['Class 0', 'Class 1']\n",
    ")\n",
    "\n",
    "plt.title('Confusion Matrix for Logistic Regression')\n",
    "plt.show()\n",
    "print(\"-\"*20)\n",
    "print(\"Confusion matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"-\"*20)\n",
    "print(class_report)\n",
    "print(\"-\"*20)\n",
    "\n",
    "num_failed = conf_matrix[0,:].sum()\n",
    "num_successful = conf_matrix[1,:].sum()\n",
    "\n",
    "print(\"Number of failed loans: \", num_failed)\n",
    "print(\"Number of successful loans: \", num_successful)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8755c2d7-4f2b-41f6-8eb5-fbbd8103c01b",
   "metadata": {},
   "source": [
    "DO THE INTERPRETATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d1ff9a-b594-462f-a0af-dd667f11c304",
   "metadata": {},
   "source": [
    "***Run the model 2nd iteration***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338d6aca-29cc-4d93-abbd-4763034f2926",
   "metadata": {},
   "source": [
    "The log reg model is ready to be run. The A log reg model will be run on both the balanced downsampled data as well as the inbalanced data, to showcase model evaluation wrt to class balance. The log reg model will use the `lbfgs` solver as it performs well on small dataset, even though it may not converge. If the model does not converge, we will check for any features with high multicollinearity,  a different solver and higher iteration count can be used. Note that instead of downsampling the set of successful loans in the dataset, a class_weight parameter can be used. Since the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befa03f6-8a5c-40f3-bdb2-4aeb9bb4d495",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca12d4-961c-4916-995f-2d86f507f215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing and training the logistic regression model\n",
    "log_reg = LogisticRegression(random_state=1,\n",
    "                             solver='lbfgs', \n",
    "                             max_iter=4000, \n",
    "                             verbose=2, #output while the model runs\n",
    "                             n_jobs=2) #use 2 cpu cores\n",
    "                             #class_weight='balanced') #weight the class to counter more frequent class\n",
    "\n",
    "log_reg.fit(X_train_transformed, y_train_bal)\n",
    "\n",
    "# Making predictions on the test data using the trained model\n",
    "y_pred_bal = log_reg.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe9c2c0-db6d-4c60-a2f2-e8f4dc2cb041",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed01482-8aee-4888-8ac6-5c91a21cb0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring the model on both train and test data\n",
    "train_score = log_reg.score(X_train_transformed, y_train_bal)\n",
    "test_score = log_reg.score(X_test_transformed, y_test)\n",
    "print(f'Score on train: {train_score}')\n",
    "print(f'Score on test: {test_score}')\n",
    "\n",
    "# Evaluating the model with confusion matrix and a classification report\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_bal)\n",
    "class_report = classification_report(y_test, y_pred_bal)\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    log_reg, \n",
    "    X_test_transformed, \n",
    "    y_test, \n",
    "    cmap='Blues', \n",
    "    display_labels=['Class 0', 'Class 1']\n",
    ")\n",
    "\n",
    "plt.title('Confusion Matrix for Logistic Regression')\n",
    "plt.show()\n",
    "print(\"-\"*20)\n",
    "print(\"Confusion matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"-\"*20)\n",
    "print(class_report)\n",
    "print(\"-\"*20)\n",
    "\n",
    "#\n",
    "num_failed = conf_matrix[0,:].sum()\n",
    "num_successful = conf_matrix[1,:].sum()\n",
    "\n",
    "print(\"Number of failed loans: \", num_failed)\n",
    "print(\"Number of successful loans: \", num_successful)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3d833f-5856-44b5-802f-a04ad485bf8e",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Score on train: 0.6504545944033352\r\n",
    "Score on test: 0.6566522963815273\n",
    "\n",
    "Our model has approximatel 65.0%% accuracy on the train se and 65.6% accuracy on the test sett. The score between the train and test set are close meaning the model fits well to unseen data, and that there is no overfitting or underfitting.,However, this will be further explored with the classification report.\n",
    " \n",
    "\n",
    "The confusion matrix above shows the counts for correctly and incorrectly predicted classes, in the format of:  \n",
    "```\n",
    "Predicted Label \n",
    "    0      1 \n",
    "+------+------+  \n",
    "| TP   |  FP  |  0\n",
    "+------+------+     True Label\n",
    "| FN   |  TN  |  1\n",
    "+------+------+  \n",
    "```\n",
    "\n",
    "\n",
    "Where,\n",
    "- **True Negative (TN):** 17,788 loans were correctly predicted as failed (class 0).\n",
    "- **False Positive (FP):** 10,071 cases were incorrectly predicted as successful (class 1) when they are actually failed (class 0).\n",
    "- **False Negative (FN):** 35,883 cases were incorrectly predicted as failed (class 0) when they are actually successful (class 1).\n",
    "- **True Positive (TP):** 70,099 cases were correctly predicted as successful (class 1).  \n",
    "\n",
    "The model showed a strong ability to discern successful from failed loans, with the majority of successful loans being accurately identified.\n",
    "For this project, the primary goal is to minimize false positives, ie instance of failed loans incorrectly predicted as successful, minimizing credit default risk. Of the 27,859 failed loans, 17,788 were correctly predicted as failed, and of the 105,982 successful loans, 70,099 were correctly predicted as successful. While the model will be tuned for precision, however, this can be adjusted based on the lenders risk appetite, allowing for a more balanced approach between granting credit and managing default risks. \n",
    "\n",
    "\n",
    "Classification Report\n",
    "- **Precision for Class 0:** 0.33, meaning when the model predicts failed, it is correct ~ 33% of the time.\n",
    "- **Recall for Class 0:** 0.64, meaning that the model correctly identifies ~ 64% of the actual failed cases.\n",
    "- **F1-Score for Class 0:** 0.44, a weighted average of precision and recall for failed loans, indicating a moderate balance between precision and recall for this class.\n",
    "- **Support for Class 0:** There are 27,859 actual occurrences of failed loans in the dataset. \n",
    "\n",
    "- **Precision for Class 1:** 0.87, suggesting that when the model predicts successful, it is correct ~ 87% of the time.\n",
    "- **Recall for Class 1:** 0.66, meaning that the model correctly identifies ~ 66% of the actual successful cases.\n",
    "- **F1-Score for Class 1:** 0.75, a weighted average of precision and recall for successful loans, indicating a strong balance between precision and recall for this the successful class.\n",
    "- **Support for Class 1:** There are 105,982 actual occurrences of successful loans in the dataset.\n",
    "\n",
    "Overall Metrics\n",
    "- **Accuracy:** 0.66, indicating that the overall, the model correctly predicts 66% of the cases.\n",
    "- **Macro Average Precision:** 0.60, the average precision across both classes.\n",
    "- **Macro Average Recall:** 0.65, the average recall across both classes.\n",
    "- **Macro Average F1-Score:** 0.59, the average F1-score across both classes.\n",
    "\n",
    "The modeloverall  perforsm better in identifying class 1 cases over class . Although the train set was balanced, the test set was left unbalanced in order to evaluate how the model would perform with real world raw data. This inbalance is likely the cause of the the low precision and high recall for class 0. Since class 0 recall measures the percentage of failed cases correctly identified, a smaller number of cases to start with could inflate this number. The low precision for class 0 could be costly as failing to identify potential failed loans is as important as identifying successful ones.  Furthermore, the relatively low F1-Score means that that both precision and recall could be improved, especially the low precision. The number of occurrences for class 0 is approximately 26% of class 1.\n",
    "\n",
    "\n",
    "On the hand, the model scored a higher `F1-Score` andr` precisio`  for class , meaning the model struck a good balance between precision and recall.\n",
    "Of the total successful loans, the model identified ~ 66%. Ideally this would be higher as to minimize any potential lost lending opportunities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f32ed64-376d-45b0-9a11-4a58a46c186f",
   "metadata": {},
   "source": [
    "We can vary the threshold to optimize the for precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88acfe9-e66a-4a55-afcf-f4011f9110c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#get the probabilities for the positive class\n",
    "y_proba = log_reg.predict_proba(X_test_transformed)[:, 1]\n",
    "\n",
    "# Vary thresholds by 0.05 from 0.05 to 1\n",
    "thresholds = np.arange(0.05, 1, 0.05)\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Apply threshold\n",
    "    y_threshold = np.where(y_proba > threshold, 1, 0)\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = precision_score(y_test, y_threshold)\n",
    "    recall = recall_score(y_test, y_threshold)\n",
    "    \n",
    "    # Append to list\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "\n",
    "# Visualize the result\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, precisions, label='Precision', marker='o')\n",
    "plt.plot(thresholds, recalls, label='Recall', marker='o')\n",
    "plt.title('Precision and Recall scores as a function of the decision threshold')\n",
    "plt.xlim(0, 1)\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fcc271-e8c3-4dfb-a61e-5ccbb2917646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "# Calculate the AUC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plotting the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c8a335-e72c-427d-b90c-26162d199f39",
   "metadata": {},
   "source": [
    "***Feature importance***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1142342-aedc-478c-a0b2-5a0aca3863a3",
   "metadata": {},
   "source": [
    "Explore which features were most useful in the prediction be inspecting their weights, keeping in mind to look for any missed leaky features remaining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb9e7eb-d851-4be5-abdc-82c43a13b631",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get the feature weights out. \n",
    "feature_weights = pd.DataFrame({\n",
    "    'Feature': preprocessor.get_feature_names_out(),\n",
    "    'Coefficient': log_reg.coef_[0]\n",
    "})\n",
    "\n",
    "# Sort the features by the absolute value of their coefficient\n",
    "feature_weights = feature_weights.sort_values(by='Coefficient', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffd6535-89a6-4c44-b71b-911be40438a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the feature weights\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.barh(feature_weights['Feature'], feature_weights['Coefficient'], color='lightblue')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96f17a8-3f9d-46e4-b9c9-00e20e5a578d",
   "metadata": {},
   "source": [
    "The categorical features home ownership and loan purpose are the most positively predictive, with the home ownership, interest rate, and loan term being the most negatively predictive. These can be interpreted as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438985d2-2d63-4aac-84ea-5db030fee8a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_odds = log_reg.coef_[0]\n",
    "odds = np.exp(log_odds)\n",
    "\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "odds_df = pd.DataFrame({'Feature': feature_names, 'LogOdds': log_odds, 'OddsRatio': odds})\n",
    "\n",
    "#sort df by OddsRatio\n",
    "odds_df = odds_df.sort_values(by='OddsRatio', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f91a82-331b-4b35-88e9-54dc531fbddf",
   "metadata": {},
   "source": [
    "We can look at the log odds ie for a unit increase in a feature, how do the odds multiply.\n",
    "For example, the if someone does not own a home, their oddsratio is \n",
    "\n",
    "Home Ownership - None: The odds of the target event are 113% higher for individuals with no home ownership compared to the baseline group, holding all other variables constant, given an odds ratio of \n",
    "\n",
    "\n",
    "\n",
    " 1.37."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adcc787-407d-4165-abc8-e1b4f97f2609",
   "metadata": {},
   "source": [
    "***3rd Iteration***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e08d8a-b1c3-468e-a1a4-210c916f431f",
   "metadata": {},
   "source": [
    "The final iteration for Log Reg. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15ffee2-66bd-496a-88d2-02cf0bd17d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate onehot encoder\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "#instantiate ordinal encoder\n",
    "ordinal_transformer = OrdinalEncoder(categories=[['Not Verified', 'Source Verified', 'Verified']])\n",
    "\n",
    "#combine into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, ['home_ownership', 'purpose', 'application_type']),\n",
    "        ('ord', ordinal_transformer, ['verification_status']),\n",
    "        ('num', StandardScaler(), make_column_selector(dtype_include=['int64','int32','float64','float32']))\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    n_jobs=2\n",
    ")\n",
    "\n",
    "#fit to the train set\n",
    "preprocessor.fit(X_train_bal)\n",
    "\n",
    "#transform the train and test sets\n",
    "X_train_transformed = preprocessor.transform(X_train_bal)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Shape of train transformed: \", X_train_transformed.shape)\n",
    "print(\"Shape of test transformed: \", X_test_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5979ae-5f22-46bc-a058-20cfb1332083",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [('scaler', StandardScaler()),\n",
    "              ('dim_redu', PCA()),\n",
    "              ('model', LogisticRegression())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04f0698-7e80-4a78-b1f9-972aeaccfab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the column transformations list + columns to which to apply\n",
    "col_transforms = [('onehot', OneHotEncoder(), ['home_ownership', 'purpose', 'application_type']),\n",
    "                  ('ordinal', OrdinalEncoder(), ['verification_status'])]\n",
    "\n",
    "preprocessor = ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c272b5-4a2b-4263-9f19-b94cf0929b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "#instantiate ordinal encoder\n",
    "ordinal_transformer = OrdinalEncoder(categories=[['Not Verified', 'Source Verified', 'Verified']])\n",
    "\n",
    "#combine into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, ['home_ownership', 'purpose', 'application_type']),\n",
    "        ('ord', ordinal_transformer, ['verification_status']),\n",
    "        ('num', StandardScaler(), make_column_selector(dtype_include=['int64','int32','float64','float32']))\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    n_jobs=2\n",
    ")\n",
    "\n",
    "# Create the column transformations list + columns to which to apply\n",
    "col_transforms = [('city_transform', OneHotEncoder(), ['home_ownership', 'purpose', 'application_type']),\n",
    "                ('review_transform', TfidfVectorizer(), 'review')]\n",
    "\n",
    "# Create the column transformer\n",
    "col_trans = ColumnTransformer(col_transforms)\n",
    "\n",
    "# Fit\n",
    "col_trans.fit(city_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeecefc-5bdf-4819-b3b1-dfe26606b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe=Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d04fce-69ea-47aa-a9cd-5cbc5321eee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{'scaler':[StandardScaler(), None, MinMaxScaler(), PowerTransformer() ], \n",
    "             'dim_redu':[PCA(), KernelPCA()],\n",
    "             'model':[LogisticRegression()],\n",
    "             'model__C':[10**i for i in range(-3,3)]},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e6592-e848-4fe7-9750-fb8a220454ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "params = ParameterGrid(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ba650d-ec62-4692-9beb-3141e4faf05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, param_grid, cv=5, verbose=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f79f32-ebb7-4647-8ceb-a8015f80693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_search = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab199cb-d799-45eb-8e08-eef7225e759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33531392-8eb3-45f7-aacd-89d53342910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_search.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682baf2-b72b-447e-a118-34fd806959d6",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13798419-ca23-4f3e-9ed9-585185b29a72",
   "metadata": {},
   "source": [
    "The logistic regression model performed quite well considering its explainability and ease of use. We achieved a 66% and 65% accuracy on our baseline model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loans_capstone",
   "language": "python",
   "name": "loans_capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
