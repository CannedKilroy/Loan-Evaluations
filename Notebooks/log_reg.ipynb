{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7737e2f6-ee61-4939-b1e9-4d86244412cc",
   "metadata": {},
   "source": [
    "# Base Model: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb8d57e-f8e2-438f-ac20-e55f148bb495",
   "metadata": {},
   "source": [
    "## Date: Nov 9, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256c83e4-ba8b-41a0-a026-0ebaeec4d1b3",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98074a2c-ee81-473a-a32e-34eb6dc688d8",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae00c66-f311-4423-ad64-b7943e0178cf",
   "metadata": {},
   "source": [
    "In this notebook, a baseline classification model will be implemented using Logistic Regression. Log reg is a good baseline as it is one of the simplest classification models, offers high explainability, and is computationally light compared some of to its counterparts. \n",
    "\n",
    "The steps:  \n",
    "After the data is read in, basic assumptions are established and checked. Any features that show high colinearity and multicollinearity are removed. Then 3 iterations of log reg will be run.\n",
    "1. Unbalanced unscaled dataset: Demonstrate how balancing the dataset affects the model's performance. \n",
    "2. Balanced scaled dataset. The first baseline model. No hyperparameter tuning\n",
    "3. Optimized model. A optimized model will be used by varying the solver, iterations, regulatization to achieve the best stable baseline log reg model.\n",
    "\n",
    "Evaluation will be focused on the 2nd and 3rd iterations as those showed significant improvements.  \n",
    "\n",
    "Note:\n",
    "- n_jobs can be increased if you have more cpu cores to speed up the modelling\n",
    "- vif calculations and the gridsearch at the end can take a few minutes to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf6673d-f070-4f0b-96bd-706e29346538",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8497c5-d4a9-4dea-b86e-238a39652efb",
   "metadata": {},
   "source": [
    "### Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7801100c-f90c-4284-b341-38ad75aa8570",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)\n",
    "   - [Table of Contents](#Table-of-contents)\n",
    "   - [Import Librarys](#Import-Librarys)\n",
    "   - [Data Dictionary](#Data-Dictionary)\n",
    "   - [Define Functions](#Define-Functions)\n",
    "   - [Load the data](#Load-the-data)\n",
    "3. [Logistic Regression Model](#Logistic-Regression-Model)\n",
    "   - [Assumptions](#Assumptions)\n",
    "   - [PreProcessing](#PreProcessing)\n",
    "   - [Modelling](#Modelling)\n",
    "   - [Evaluation](#Evaluation)\n",
    "8. [Conclusion](#Conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8125e564-2b37-4bdb-a886-123ed781112a",
   "metadata": {},
   "source": [
    "### Import Librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cbeb4b-1a60-4ed0-860d-f4b52aa2d41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from joblib import dump\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from helpers import display_corr_heatmap, data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25946e1f-a975-4d39-a161-bd37d2646a21",
   "metadata": {},
   "source": [
    "### Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d755ac4-e7b0-43c2-becc-6542083efaad",
   "metadata": {},
   "source": [
    "Display the data dictionary. Reminder to place it under the `/Data/Lending_club/` directory like the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeaf3a2-4536-4dd2-b6a0-4637b24f788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249e4929-f85e-492b-a8ba-b9a4e5772d02",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e07d49-3123-43f8-92e6-d650d28887d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the relative path to the file\n",
    "parquet_file_path = Path('../Data/Lending_club/model_cleaned')\n",
    "\n",
    "try:\n",
    "    # Read the parquet file\n",
    "    loans_df = pd.read_parquet(parquet_file_path)\n",
    "except FileNotFoundError as e:\n",
    "    print(e.args[1])\n",
    "    print('Check file location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a0c361-0ecf-4d29-b758-8f938f49c20e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loans_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bc45ce-c611-4dd2-93db-d3090f11b343",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c95760f-a9ab-428a-bb0f-a4b864a7e813",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731264c4-c62d-43a6-bd94-5deb112bf41b",
   "metadata": {},
   "source": [
    "### Assumptions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b262f00-2669-4ea2-b17b-058e6083bc9d",
   "metadata": {},
   "source": [
    "Before we can start modeling, some base assumptions must be met in order to use a log reg model.   \n",
    "These include:  \n",
    "* **Binary Outcome:** The dependent variable is a binary. This is met as loan status has been encoded as 1 and 0\n",
    "* **Independence:** It is reasonable to assume loans are independent. Without identifiable information, there is not way of knowing from the dataset whether a borrower has applied for multiple loans as the member_id data has been removed by lendingclub.\n",
    "* **No collinearity / multicollinearity.** Can lead to unstable coefficient estimates and interpretation difficulty. This will be checked\n",
    "* **Sufficiently Large sample size:** More than 100,000 loans. This is met"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d3d311-cddd-46da-b6a3-3a0e9561c855",
   "metadata": {},
   "source": [
    "### Colinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6400cb8-463f-4b7d-8f9e-ccf79d6ddb60",
   "metadata": {},
   "source": [
    "Plot a correlation heatmap for the remaining features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b013db7-d686-4335-af78-47b99b248c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_corr_heatmap(loans_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee30003-a721-404f-babd-bc5d91fd451d",
   "metadata": {},
   "source": [
    "There are some features with high colinearity, such as `open_acc`, `total_acc`, etc. These are primarily from the applicants credit report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce377bcb-c34b-4cfe-b1d1-ec0dfb2c988c",
   "metadata": {},
   "source": [
    "### Collinearity / Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea894f-419d-464e-84ed-2e35e679cbd2",
   "metadata": {},
   "source": [
    "Check for multicollinearity and collineartiy before we split the data or encode categorical variables. First check for multicollinearity using Variance Inflation Factor (VIF). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c85f7f5-9dc3-4970-8c46-ca765670cec6",
   "metadata": {},
   "source": [
    "Create a df to hold the vif scores. Calculate the vif scores for each feature and place in the dataframe. This may take a few moments as it is running a linear regression for each feature against all others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50b4271-3a5a-45a8-94f9-19bab920a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#define a vif threshold. 5 and 10 are standard \n",
    "vif_cutoff = 10\n",
    "\n",
    "#select only numeric features\n",
    "numeric_df = loans_df.select_dtypes(include=[np.number])\n",
    "\n",
    "#create a dataframe to hold the vif scores for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = numeric_df.columns\n",
    "\n",
    "#calculate the vif\n",
    "print('Running vif calculations...')\n",
    "vif_data['VIF'] = [variance_inflation_factor(numeric_df.values, i) for i in range(len(numeric_df.columns))]\n",
    "print('Finished vif calculations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd81e97f-e3d0-460d-aadb-0b31226167c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vif_data.sort_values(by=['VIF'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66778510-20b9-43f6-9def-b2d67e9f920f",
   "metadata": {},
   "source": [
    "Create a list of the columns with a vif greater than the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03d5236-a6fa-4f08-b101-40c303fcc0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_vif_columns = vif_data[vif_data['VIF'] > vif_cutoff]['feature'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616ea539-1327-4573-9628-c3af467756c1",
   "metadata": {},
   "source": [
    "Before dropping the features with high vif, inspect them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2d9eda-d873-4804-abf9-7e9deddfd56a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(high_vif_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2cf50d-a133-4542-87b7-1e84789403eb",
   "metadata": {},
   "source": [
    "We will leave `loan_amnt`, `term`, `int_rate`, as these are key features of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0788b5be-564e-4b8b-8232-8c82c79397ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop features with high VIF\n",
    "filtered_high_vif_columns = [feature for feature in high_vif_columns if feature not in ['loan_amnt', 'term', 'int_rate']]\n",
    "loans_df.drop(columns = filtered_high_vif_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3e7a22-8da7-4e58-afb3-89ea304a3150",
   "metadata": {},
   "source": [
    "The remaining features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3bdfff-8e2a-41e3-ac42-34cc4b7576e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_df.head(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780db858-d49e-4d4f-9704-ac3e9102c8a5",
   "metadata": {},
   "source": [
    "***Collinearity***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062ac3d3-002a-4608-a34a-3ab4ce9551f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_corr_heatmap(loans_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5196dfbf-5195-4773-8b8b-d700f06b5df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_df.drop(columns = ['num_tl_op_past_12m'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e42b7617-8fca-424e-89ed-f5547428ffd3",
   "metadata": {},
   "source": [
    "There are still some high correlations between variables specifically `num_tl_op_past_12m`, the number of accounts opened in the past 12 months, with\n",
    "`acc_open_past_24mths`, the number of trades opened the last 24 months. Although the correlation is moderately high, there is no redundancy as they represent different aspects of the credit report, so both will be kept. There are no major features highly correlated with the target variable.  \n",
    "All the assumptions have now been met"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a91e07-d156-424d-8e5f-c6149a3e34b6",
   "metadata": {},
   "source": [
    "### PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f379563b-3fb6-4de2-b8dc-a86e594d04b1",
   "metadata": {},
   "source": [
    "***Train Test Split***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c767a32e-3e79-4bdc-82b7-2dc6f3434252",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X = loans_df.drop(columns=['loan_status'], inplace=False)\n",
    "y = loans_df['loan_status']\n",
    "\n",
    "# Split into train and test sets. Stratify to ensure any inbalance is preserved as in the original data and not lost by random chance. \n",
    "# Create seperate splits for the unbalanced and balanced model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=11, stratify=y)\n",
    "unbal_X_train, unbal_X_test, unbal_y_train, unbal_y_test = train_test_split(X, y, test_size=0.3, random_state=11, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50740bc5-c66e-42ee-9169-d3ae76fc6ee6",
   "metadata": {},
   "source": [
    "***Data Inbalance***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e7c66c-887e-402a-af9a-4c071707f50b",
   "metadata": {},
   "source": [
    "As shown in EDA, there is a large inbalance between the number of successful loans (class 1) and failed loans (class 0), approximately 80/20. Since the dataset is sufficiently large, it is acceptable to downsample the instances of class 1 to equal class 0. Balancing the dataset reduces the risk of any bias introduced to a single class simply due to its frequency in the dataset.  \n",
    "\n",
    "This also has the added advantage of giving a more manageable dataset size. However, if computation power is not an issue, then more failed loans could be sampled from the original dataset, and / or synthetic data created for the minority class using SMOTE.  \n",
    "More information can be found here:  \n",
    "https://towardsdatascience.com/smote-fdce2f605729"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f238a59d-9a4e-434b-8a60-4b6e0016b21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of class 1 examples before:', X_train[y_train == 1].shape[0])\n",
    "\n",
    "# Downsample majority class without replacement to the same size of the minority class\n",
    "X_downsampled, y_downsampled  = resample(X_train[y_train == 1],\n",
    "                                   y_train[y_train == 1],\n",
    "                                   replace=False,\n",
    "                                   n_samples=X_train[y_train == 0].shape[0],\n",
    "                                   random_state=1)\n",
    "\n",
    "print('\\nNumber of class 1 examples after:', X_downsampled.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed11a6f-9e27-46f2-818b-adf17e0ef779",
   "metadata": {},
   "source": [
    "Can now combine with the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2389e754-7959-4c90-b399-89494fe06bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the downsampled successful loans with the failed loans. Will keep as a df since changing to \n",
    "X_train_bal = pd.concat([X_train[y_train == 0], X_downsampled])\n",
    "y_train_bal = np.hstack((y_train[y_train == 0], y_downsampled))\n",
    "\n",
    "print(\"New X_train shape: \", X_train_bal.shape)\n",
    "print(\"New y_train shape: \", y_train_bal.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f131f6d8-c167-46a1-9b1e-74f63ef27a25",
   "metadata": {},
   "source": [
    "***Inspect Categorical Features***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1139764-8708-4399-9409-95b5e303ab98",
   "metadata": {},
   "source": [
    "Inspect whether the categorical features are ordinal or nominal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82adf0b5-4424-4ee9-a4a6-c0c364d85033",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = X_train_bal.select_dtypes('object').columns.tolist()\n",
    "display(categorical_columns)\n",
    "categorical_columns.remove('verification_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5579c0db-f0f0-45d0-8348-0e4c2c7a08ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bal['verification_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff8acb7-9d0d-45a3-87db-37275b0cf594",
   "metadata": {},
   "source": [
    "The feature `verification_status` will be **ordinal encoded** since loan applications with verified income information should be weighted higher than those that are only source verified or unverified. The other categorical features can be onehot encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d110f555-64a8-403b-8df0-60cce98f9efd",
   "metadata": {},
   "source": [
    "***Column Transformation for 1st iteration***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3185c022-b585-4675-9a77-1042f48cf51e",
   "metadata": {},
   "source": [
    "Just encode the categorical variables as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeef741-d738-4e8a-9948-ce04c9db590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate onehot encoder\n",
    "unbal_categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "#instantiate ordinal encoder\n",
    "unbal_ordinal_transformer = OrdinalEncoder(categories=[['Not Verified', 'Source Verified', 'Verified']])\n",
    "\n",
    "#instantiate the column transformer\n",
    "unbal_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', unbal_categorical_transformer, ['home_ownership', 'purpose', 'application_type']),\n",
    "        ('ord', unbal_ordinal_transformer, ['verification_status']),\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    n_jobs=2 #use 2 cpu cores for greater speed\n",
    ")\n",
    "\n",
    "#fit to the train set\n",
    "unbal_preprocessor.fit(unbal_X_train)\n",
    "\n",
    "#transform the train and test sets\n",
    "unbal_X_train_transformed = unbal_preprocessor.transform(unbal_X_train)\n",
    "unbal_X_test_transformed = unbal_preprocessor.transform(unbal_X_test)\n",
    "\n",
    "print(\"Shape of train transformed: \", unbal_X_train_transformed.shape)\n",
    "print(\"Shape of test transformed: \",  unbal_X_test_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7779894b-ebd8-4bf7-b713-d73ecb891bf1",
   "metadata": {},
   "source": [
    "***Column Transformation for 2nd Iteration***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216f5e8-2962-4447-9405-ab9789f42ade",
   "metadata": {},
   "source": [
    "For the second iteration, a standard scaler is fit as well. Although log reg is not a distance based model, it can aid in model performance by reducing the size of the parameter space, allowing the model to converge more easily.  \n",
    "More information can be found here:  \n",
    "https://forecastegy.com/posts/does-logistic-regression-require-feature-scaling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cac96be-04ff-4479-a4fd-4d1adb44ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate onehot encoder\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "#instantiate ordinal encoder\n",
    "ordinal_transformer = OrdinalEncoder(categories=[['Not Verified', 'Source Verified', 'Verified']])\n",
    "\n",
    "#combine into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, ['home_ownership', 'purpose', 'application_type']),\n",
    "        ('ord', ordinal_transformer, ['verification_status']),\n",
    "        ('num', StandardScaler(), make_column_selector(dtype_include=['int64','int32','float64','float32']))\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    n_jobs=2\n",
    ")\n",
    "\n",
    "#fit to the train set\n",
    "preprocessor.fit(X_train_bal)\n",
    "\n",
    "#transform the train and test sets\n",
    "X_train_transformed = preprocessor.transform(X_train_bal)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Shape of train transformed: \", X_train_transformed.shape)\n",
    "print(\"Shape of test transformed: \", X_test_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff635be9-272a-42de-9f19-2d39889574d2",
   "metadata": {},
   "source": [
    "***Run the model 1st iteration***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2284f0db-7e03-47cd-ab20-4efedc3be8eb",
   "metadata": {},
   "source": [
    "The log reg models are ready to be run. A log reg model will be run on both the balanced downsampled data as well as the inbalanced data, to showcase model evaluation wrt to class balance. The log reg model will use the `lbfgs` solver as it performs well on small dataset, even though it may not converge. If the model does not converge, we will check for any features with high multicollinearity, and try a different solver and/ or higher iteration count.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce2f0b0-db80-4773-916d-933d83d39bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing and training the logistic regression model\n",
    "unbal_log_reg = LogisticRegression(random_state=1,\n",
    "                             solver='lbfgs', \n",
    "                             max_iter=4000, \n",
    "                             verbose=2, #output while the model runs\n",
    "                             n_jobs=2) #use 2 cpu cores\n",
    "                             #class_weight='balanced') #weight the minority class to counter inbalance also works\n",
    "\n",
    "unbal_log_reg.fit(unbal_X_train_transformed, unbal_y_train)\n",
    "\n",
    "# Making predictions on the test data using the trained model\n",
    "unbal_y_pred = unbal_log_reg.predict(unbal_X_test_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876c0af6-212d-4663-8a63-0a7271bb2eaa",
   "metadata": {},
   "source": [
    "***Score the 1st iteration model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7072ba48-77b8-4481-84b9-5fe99b8104a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the model on both train and test data\n",
    "unbal_train_score = unbal_log_reg.score(unbal_X_train_transformed, unbal_y_train)\n",
    "unbal_test_score = unbal_log_reg.score(unbal_X_test_transformed, unbal_y_test)\n",
    "\n",
    "conf_matrix = confusion_matrix(unbal_y_test, unbal_y_pred)\n",
    "class_report = classification_report(unbal_y_test, unbal_y_pred)\n",
    "\n",
    "# Display the Confusion Matrix\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    unbal_log_reg, \n",
    "    unbal_X_test_transformed, \n",
    "    unbal_y_test, \n",
    "    cmap='Blues', \n",
    "    display_labels=['Class 0', 'Class 1']\n",
    ")\n",
    "plt.title('Confusion Matrix for Logistic Regression')\n",
    "plt.show()\n",
    "\n",
    "#################################\n",
    "# Calculate the ROC curve\n",
    "\n",
    "# Calculate the probability scores\n",
    "probs = unbal_log_reg.predict_proba(unbal_X_test_transformed)\n",
    "\n",
    "# Keep only the positive class (class 1)\n",
    "probs = probs[:, 1]\n",
    "\n",
    "# Pull out the fprs, tprs, and thresholds\n",
    "fprs, tprs, thresholds = roc_curve(unbal_y_test, probs)\n",
    "roc_auc = roc_auc_score(unbal_y_test, probs)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fprs, tprs, color='darkorange',\n",
    "         lw=2, label='AUC = %0.2f' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('ROC Curve for Loan Default Prediction')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "print(f'Area under curve (AUC):{roc_auc}')\n",
    "#################################\n",
    "\n",
    "# Print the confusion matrix and Classification Report\n",
    "print(\"-\"*20)\n",
    "print(\"Confusion matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"-\"*20)\n",
    "print(class_report)\n",
    "print(\"-\"*20)\n",
    "\n",
    "print(f'Score on train: {unbal_train_score}')\n",
    "print(f'Score on test: {unbal_test_score}')\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8755c2d7-4f2b-41f6-8eb5-fbbd8103c01b",
   "metadata": {},
   "source": [
    "As a scaled balanced dataset will almost guaranteed perform better, a brief evaluation on the confusion matrix and classification report will be sufficient to justify balancing the dataset. At first glance, the model seems to have performed well with a ~ 78.8% accuracy for both train and test sets. However, when considered with the class inbalance of 73.7%, a 78.7% accuracy is a small improvement over random guesswork. This inbalance results in the model predicting class 1 more frequently, resulting in a very high recall score for successful loans. Of all the successful loans, 99% were correctly identified, meaning there were very successful lending opportunities were missed. However, this came at a cost. For failed loans, the precision is very low, with a terrible recall. Indiscriminately predicting many loans to be successful led to a high number of false positives, with failed loans being misclassified as successful. This would result in a much higher credit risk. Of all the failed loans, only 3% were correctly identified (recall), with 42% of loans predicted as failed, actually failed (precision). The low precision would result in many missed lending oppertunites, with the low recall resulting in a very high credit risk. \n",
    "\n",
    "The weighted combined f1 score, an average for the precision and recall, is moderate at 0.71, showing overall a weak differentiation between successful and failed loans. However, this metric has less value considering the class inbalance. \n",
    "\n",
    "The ROC curve shows that the model is doing better than random choice (the horizontal line). The AUC of of 0.64 implies that when randomly selecting one positive and one negative example, the model will correctly assign a higher probability of being positive to the positive example about 64% of the time. Ideally, an AUC score closer to 1 would be preferred since that requires a higher TPR and lower FPR. \n",
    "\n",
    "Overall, this model correctly predicted many successful loans, at the cost of classifying many bad loans as successful. \n",
    "In the next iteration with a balanced scaled dataset, the aim will be to maintain a comparable precision, while minimizing false positives, resulting in a better balance between real credit opportunities and managing default risks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d1ff9a-b594-462f-a0af-dd667f11c304",
   "metadata": {},
   "source": [
    "***Run the model 2nd iteration***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca12d4-961c-4916-995f-2d86f507f215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing and training the logistic regression model\n",
    "log_reg = LogisticRegression(random_state=1,\n",
    "                             solver='lbfgs', \n",
    "                             max_iter=4000, \n",
    "                             verbose=2, #output while the model runs\n",
    "                             n_jobs=2) #use 2 cpu cores\n",
    "\n",
    "log_reg.fit(X_train_transformed, y_train_bal)\n",
    "\n",
    "# Making predictions on the test data using the trained model\n",
    "y_pred_bal = log_reg.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe9c2c0-db6d-4c60-a2f2-e8f4dc2cb041",
   "metadata": {},
   "source": [
    "***Score the 2nd iteration model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed01482-8aee-4888-8ac6-5c91a21cb0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the model on both train and test data\n",
    "train_score = log_reg.score(X_train_transformed, y_train_bal)\n",
    "test_score = log_reg.score(X_test_transformed, y_test)\n",
    "\n",
    "# Evaluating the model with confusion matrix and a classification report\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_bal)\n",
    "class_report = classification_report(y_test, y_pred_bal)\n",
    "\n",
    "#Display the confusion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    log_reg, \n",
    "    X_test_transformed, \n",
    "    y_test, \n",
    "    cmap='Blues', \n",
    "    display_labels=['Class 0', 'Class 1']\n",
    ")\n",
    "plt.title('Confusion Matrix for Logistic Regression')\n",
    "plt.show()\n",
    "\n",
    "print(\"-\"*20)\n",
    "print(\"Confusion matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"-\"*20)\n",
    "print(class_report)\n",
    "print(\"-\"*20)\n",
    "\n",
    "print(f'Score on train: {train_score}')\n",
    "print(f'Score on test: {test_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3d833f-5856-44b5-802f-a04ad485bf8e",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The model has~ 65.0%% accuracy on the train ste and~ 65.6% accuracy on the test stt. The score between the train and test set are clos,  meaning the model generalizes  well to unseen data, and that there is no overfitting or underfitting ,However, this will be further explored with the classification report \n",
    "\n",
    "The confusion matrix above shows the counts for correctly and incorrectly predicted classes, in the format of:  \n",
    "```\n",
    "Predicted Label \n",
    "    0      1 \n",
    "+------+------+  \n",
    "| TP   |  FP  |  0\n",
    "+------+------+     True Label\n",
    "| FN   |  TN  |  1\n",
    "+------+------+  \n",
    "```\n",
    "\n",
    "\n",
    "Where,\n",
    "- **True Negative (TN):**177738 loans were correctly predicted as failed (class 0).\n",
    "- **False Positive (FP):**100861 cases were incorrectly predicted as successful (class 1) when they are actually failed (class 0).\n",
    "- **False Negative (FN):**35876 3 cases were incorrectly predicted as failed (class 0) when they are actually successful (class 1).\n",
    "- **True Positive (TP):**701069 cases were correctly predicted as successful (class 1).  \n",
    "\n",
    "The model showed a strong ability to discern successful from failed loans, with the majority of successful loans being accurately identified.\n",
    "For this project, the primary goal is to minimize false positives, ie instance of failed loans incorrectly predicted as successful, minimizing credit default risk. Of the 27,859 failed loans, 17,788 were correctly predicted as failed, and of the 105,982 successful loans, 70,099 were correctly predicted as successful. While the model will be tunedto favor  precision, however, this can be adjusted based on the lenders risk appetite, allowing for a more balanced approach between granting credit and managing default risks.\n",
    "\n",
    "\n",
    "Classification Report\n",
    "- **Precision for Class 0:** 0.33, meaning when the model predicts failed, it is correct ~ 33% of the time.\n",
    "- **Recall for Class 0:** 0.64, meaning that the model correctly identifies ~ 64% of the actual failed cases.\n",
    "- **F1-Score for Class 0:** 0.44, a weighted average of precision and recall for failed loans, indicating a moderate balance between precision and recall for this class.\n",
    "- **Support for Class 0:** There are 27,859 actual occurrences of failed loans in the dataset. \n",
    "\n",
    "- **Precision for Class 1:** 0.87, suggesting that when the model predicts successful, it is correct ~ 87% of the time.\n",
    "- **Recall for Class 1:** 0.66, meaning that the model correctly identifies ~ 66% of the actual successful cases.\n",
    "- **F1-Score for Class 1:** 0.75, n  average of precision and recall for successful loans, indicating a strong balance between precision and recall for the successful class.\n",
    "- **Support for Class 1:** There are 105,982 actual occurrences of successful loans in the dataset.\n",
    "\n",
    "Overall Metrics\n",
    "- **Accuracy:** 0.66, indicating that the overall, the model correctly predicts 66% of the cases.\n",
    "- **Macro Average Precision:** 0.60, the average precision across both classes.\n",
    "- **Macro Average Recall:** 0.65, the average recall across both classes.\n",
    "- **Macro Average F1-Score:** 0.59, the average F1-score across both classes  .\n",
    "Compared to the previous iteration, this model has made significant improvement. The precision for class 1 increased from 0.79 -> 0.86, meaning fewer bad loans were misclassified. The increase in precision came at the cost of recall, which decreased from 0.99 to 0.66. Fewer good lending opportunities were captured. However, the recall for failed loans increased from 0.03 to 0.64, meaning the model identified many more bad loans correctly. The f1 score decreased slightly, but the AUC increased. The model improved in distinguishing between both classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f32ed64-376d-45b0-9a11-4a58a46c186f",
   "metadata": {},
   "source": [
    "***Threshold Optimization***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88acfe9-e66a-4a55-afcf-f4011f9110c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#get the probabilities for the positive class\n",
    "y_proba = log_reg.predict_proba(X_test_transformed)[:, 1]\n",
    "\n",
    "# Vary thresholds by 0.05 from 0.05 to 1\n",
    "thresholds = np.arange(0.05, 1, 0.05)\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Apply threshold\n",
    "    y_threshold = np.where(y_proba > threshold, 1, 0)\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = precision_score(y_test, y_threshold)\n",
    "    recall = recall_score(y_test, y_threshold)\n",
    "    \n",
    "    # Append to list\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "\n",
    "# Visualize the result\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, precisions, label='Precision', marker='o')\n",
    "plt.plot(thresholds, recalls, label='Recall', marker='o')\n",
    "plt.title('Precision and Recall scores as a function of the decision threshold')\n",
    "plt.xlim(0, 1)\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6db3804-0fa7-48bc-a361-112a1cfedbd8",
   "metadata": {},
   "source": [
    "As mentioned above, precision is preferred to recall for this project, as we want to only extend credit to worthy borrowers that are less likely to default, even if that comes at the cost of some missed lending opportunities. A threshold of 0.4 or 0.5 is appropriate here as afterward recall begins to steeply decrease. Once again, this can be varied based on lender risk tolerances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b06754-1a1f-438a-a9c0-5f1c29d064dd",
   "metadata": {},
   "source": [
    "***ROC Curve***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fcc271-e8c3-4dfb-a61e-5ccbb2917646",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "# Calculate the AUC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plotting the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c598e1ca-0600-415c-852d-96fc3468b76d",
   "metadata": {},
   "source": [
    "The increased AUC score means for a certain threshold, the FPR to TPR has been improved, meaning the model is able to better discern betweem classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c8a335-e72c-427d-b90c-26162d199f39",
   "metadata": {},
   "source": [
    "***Feature importance***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1142342-aedc-478c-a0b2-5a0aca3863a3",
   "metadata": {},
   "source": [
    "Explore which features were most useful in the prediction be inspecting their weights, keeping in mind to look for any missed leaky features remaining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb9e7eb-d851-4be5-abdc-82c43a13b631",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get the feature weights out. \n",
    "feature_weights = pd.DataFrame({\n",
    "    'Feature': preprocessor.get_feature_names_out(),\n",
    "    'Coefficient': log_reg.coef_[0]\n",
    "})\n",
    "\n",
    "# Sort the features by the absolute value of their coefficient\n",
    "feature_weights = feature_weights.sort_values(by='Coefficient', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffd6535-89a6-4c44-b71b-911be40438a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the feature weights\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.barh(feature_weights['Feature'], feature_weights['Coefficient'], color='lightblue')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96f17a8-3f9d-46e4-b9c9-00e20e5a578d",
   "metadata": {},
   "source": [
    "The categorical features home ownership and loan purpose are the most positively predictive, with the home ownership, interest rate, and loan term being the most negatively predictive.  \n",
    "Note:  \n",
    "- \"cat prefix indicates it was a categorical variable, and the \"remainder\" prefix can be ignored. It is left over from the columntransformer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd200fa7-1922-4ca6-9eb6-ed9382c69d36",
   "metadata": {},
   "source": [
    "***Log odds***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438985d2-2d63-4aac-84ea-5db030fee8a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_odds = log_reg.coef_[0]\n",
    "odds = np.exp(log_odds)\n",
    "\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "odds_df = pd.DataFrame({'Feature': feature_names, 'LogOdds': log_odds, 'OddsRatio': odds})\n",
    "\n",
    "#sort df by OddsRatio\n",
    "odds_df = odds_df.sort_values(by='OddsRatio', ascending=False)\n",
    "odds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f91a82-331b-4b35-88e9-54dc531fbddf",
   "metadata": {},
   "source": [
    "We can look at the log odds ie for a unit increase in a feature, how do the odds multiply.\n",
    "For example,\n",
    "\n",
    "No Home Ownership: The odds of a successful loan are (2.36-1)X100=136% higher whene ownership compared to the bas (some form of house ownership)eline group, holding all other variabl.atio of t. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adcc787-407d-4165-abc8-e1b4f97f2609",
   "metadata": {},
   "source": [
    "***Final Iteration***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590b03af-e40c-4095-88f7-083fbf3e084e",
   "metadata": {},
   "source": [
    "For the final iteration, a automated process will be used to fine tune the hyperparameters. This will ensure no steps are missed, no accidental data leakage is made, and is generally more efficient. As found in the last iteration, log reg trained on a balanced scaled dataset out performed a log reg model trained on a unbalanced unscaled dataset. The hyperparameters to be tuned will be:  \n",
    "1. C value: The inverse of the strength or regulatization. \n",
    "2. Penalty type: L1 (lasso) and L2 (ridge). Although there was no overfitting, regularization might still help with feature selection and model stability\n",
    "3. Solver: Although there was no issue with convergence above, different solvers allow for different penalties. The SAGA solver will be used as it allows for all the different penalties, is efficient on large datasets, and recommended by sklearn.\n",
    "\n",
    "The column transformers for encoding the categorical variables are created and passed into a pipeline. The pipeline allows these preprocessing transformations to be bundled together into a single object, carried out sequentially, and helps avoid any data leakage. A hyperparameter grid is then established, allowing GridSearchCV to run an exhaustive check on all the different combinations of hyperparameters by traversing this table. This process of checking all the combinations of hyperparameters is then repeated 5 times for each \"fold\" in the 5 fold cross-validation. The training data is split in k folds, in this case 5, smaller sets with the model being trained and validated on each k times. This gives a more reliable estimation than a single split due to that fact that when splitting randomly, certain hyperparameters might perform better just based on the happenstance of data included in that set. This can be mitigated if it repeated across the entire dataset, ensuring full coverage, and then averaged across all the folds.  \n",
    "\n",
    "Note:  \n",
    "- This is very time consuming\n",
    "- If you selected different data and find the model is not converging, pass a high max_iter as well  \n",
    "More information:  \n",
    "https://datascience.stackexchange.com/questions/108792/why-is-the-k-fold-cross-validation-needed  \n",
    "https://datascience.stackexchange.com/questions/43972/when-should-i-use-standardscaler-and-when-minmaxscaler\n",
    "https://stackoverflow.com/questions/73479995/solver-lbfgs-supports-only-l2-or-none-penalties-got-l1-penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15ffee2-66bd-496a-88d2-02cf0bd17d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from tempfile import mkdtemp\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "#instantiate categorical encoders\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "ordinal_transformer = OrdinalEncoder(categories=[['Not Verified', 'Source Verified', 'Verified']])\n",
    "\n",
    "#cache results for increased speed\n",
    "cachedir = mkdtemp()\n",
    "\n",
    "#combine into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, ['home_ownership', 'purpose', 'application_type']),\n",
    "        ('ord', ordinal_transformer, ['verification_status']),\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    n_jobs=2) # 2 cpu cores\n",
    "\n",
    "#changed max iter since default 100 is quite low\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection', SelectFromModel(LogisticRegression(penalty='l1', solver='saga'))),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, solver='saga', random_state=1))], memory = cachedir)\n",
    "\n",
    "#define the hyperparameters grid to search\n",
    "'''\n",
    "param_grid = [\n",
    "    {\n",
    "        'classifier__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "        'classifier__C': [0.001, 0.007, 0.01, 0.03, 0.1],\n",
    "        'classifier__l1_ratio': [0.5]  # Only needed if elasticnet is used\n",
    "    },\n",
    "    {\n",
    "        'classifier__penalty': ['none'],\n",
    "        'classifier__C': [1]  # 'C' is not used when penalty is 'none'\n",
    "    }\n",
    "]\n",
    "'''\n",
    "param_grid = [\n",
    "    {\n",
    "        'classifier__penalty': ['l1', 'l2'],\n",
    "        'feature_selection__estimator__penalty': ['l1'],\n",
    "        'classifier__C': [0.005, 0.006, 0.007, 0.008],\n",
    "        'feature_selection__threshold': ['mean', 'median']  # Example thresholds\n",
    "        # No 'l1_ratio' should be specified here\n",
    "    },\n",
    "    {\n",
    "        'classifier__penalty': ['none'],\n",
    "        'feature_selection__threshold': ['mean', 'median'],  # Example thresholds\n",
    "        'classifier__C': [1]  # 'C' is not used when penalty is 'none'\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Setup the GridSearchCV object\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, verbose=10, n_jobs=2)\n",
    "\n",
    "#fit to the balanced data. Any transformations are handled by the pipeline\n",
    "grid_search.fit(X_train_bal, y_train_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a054525-b380-4d6b-a3ca-13028572f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model object\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77147d3d-e5ad-4e85-a6a7-65d417623158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9463243-cf94-475d-bcda-997ae92ff5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean test score for each CV fold\n",
    "grid_search.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916b36c5-6601-4193-a62d-a450f968ea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the score\n",
    "grid_search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48fb276-7e99-4299-8e65-214de9f3f85e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(grid_search.cv_results_).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29295f1-18ec-4dd1-9c63-f2e4d96b0566",
   "metadata": {},
   "source": [
    "***Save the best model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1456aa-288e-448d-8763-57d04e75f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#relative path to models folder\n",
    "model_file_path = Path('../Models/best_logistic_regression_model.joblib')\n",
    "dump(best_model, model_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce03b7ab-f293-4d8c-910f-fd8aa666a7fd",
   "metadata": {},
   "source": [
    "***Evaluate the best model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd81a33-64c8-41bd-ac8d-46f37e02454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, roc_curve, auc, RocCurveDisplay\n",
    "\n",
    "grid_search.cv_results_['mean_test_score']\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(class_report)\n",
    "\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\n",
    "disp.plot()\n",
    "plt.title('Confusion Matrix for Logistic Regression')\n",
    "plt.show()\n",
    "\n",
    "###########################################\n",
    "# ROC CURVE AUC CURVE\n",
    "\n",
    "# Get the fpr, tpr, thresholds from the probability predict\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "# Calculate the AUC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plotting the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd752ac-e05d-47a2-ba2e-cbf014136113",
   "metadata": {},
   "source": [
    "Using the gridsearch, there is no improvement over our second iteration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a2d69f-a09a-48c8-a7e9-0db2077ce41a",
   "metadata": {},
   "source": [
    "***Feature importance***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dfb9cb-b887-42e0-afb0-7eb3082fd2fc",
   "metadata": {},
   "source": [
    "***Feature Importance Final Iteration***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28def4d-3e13-4ae9-886d-c3c7738a8900",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db87bf3d-037b-410d-bdb4-a41cca2af896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to 1D array\n",
    "classifier_coefs = best_model.named_steps['classifier'].coef_.flatten()\n",
    "\n",
    "#which features were kept\n",
    "selected_features_mask = best_model.named_steps['feature_selection'].get_support()\n",
    "\n",
    "# Apply the mask to the transformed feature names to get the selected feature names\n",
    "selected_feature_names = transformed_feature_names[selected_features_mask]\n",
    "\n",
    "# Use np.where to find indices of features that were not selected\n",
    "dropped_feature_indices = np.where(selected_features_mask == False)[0]\n",
    "\n",
    "# Get the names of the dropped features by indexing into the transformed feature names\n",
    "dropped_feature_names = transformed_feature_names[dropped_feature_indices]\n",
    "\n",
    "# Print out the dropped features\n",
    "print(\"Dropped Features:\", dropped_feature_names)\n",
    "\n",
    "feature_weights_gridsv = pd.DataFrame({\n",
    "        'Feature': selected_feature_names,\n",
    "        'Coefficient': classifier_coefs\n",
    "    })\n",
    "# Sort the features by the absolute value of their coefficient\n",
    "feature_weights_gridsv = feature_weights_gridsv.sort_values(by='Coefficient', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1de7fbe-7a58-463f-b9c5-21a7b58de49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the feature weights\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.barh(feature_weights_gridsv['Feature'], feature_weights_gridsv['Coefficient'], color='lightblue')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109ab5b3-c18b-4599-ae08-8f49983626d5",
   "metadata": {},
   "source": [
    "The model achieved the same scores, however many features with little or no predictive value were dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6685de-a8ee-48a6-a0c7-93697388debd",
   "metadata": {},
   "source": [
    "***Vary the threshold***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ef3079-5ed0-4778-ae1d-554a35715711",
   "metadata": {},
   "source": [
    "Plot the precision recall for different thresholds again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740ad4ae-8ec0-4cb5-bf7f-1d2554693aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Vary thresholds by 0.05 from 0.05 to 1\n",
    "thresholds = np.arange(0.05, 1, 0.05)\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Apply threshold\n",
    "    y_threshold = np.where(y_proba > threshold, 1, 0)\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = precision_score(y_test, y_threshold)\n",
    "    recall = recall_score(y_test, y_threshold)\n",
    "    \n",
    "    # Append to list\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "\n",
    "# Visualize the result\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, precisions, label='Precision', marker='o')\n",
    "plt.plot(thresholds, recalls, label='Recall', marker='o')\n",
    "plt.title('Precision and Recall scores as a function of the decision threshold')\n",
    "plt.xlim(0, 1)\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc01482-72e8-445f-9e49-f66f28500fcf",
   "metadata": {},
   "source": [
    "The Precision and Recall curves are the same as the previous iteration. A threshold of 0.4, 0.5 is suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a01de8-ee9e-4f88-a4d8-512311788c3b",
   "metadata": {},
   "source": [
    "***Look for common factors in False Positives***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34b0e6f-06c6-4704-9c01-40ef2d06f330",
   "metadata": {},
   "source": [
    "We can examine the false positives next to pinpoint possible problem features for the model, and to see what feature values were potentially leading to the incorrect predictions.\n",
    "\n",
    "Note: **This is done on the test set, and therefore we CANNOT run another iteration using this information or retroactively change parameters in the pipeline, as that would be leaking data from the test set and could result in inflated metrics and over fitting. This is simply explanatory.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eafdc47-fe05-41ea-8ed5-20e24e9740b5",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/50094999/how-to-return-an-array-of-false-positives-from-a-confusion-matrix-in-scikit-lear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08925fe8-2b4c-45f1-b143-892dcbd57e44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Transform X_test so it matches the feature names\n",
    "X_test_transformed = best_model.named_steps['preprocessor'].transform(X_test)\n",
    "\n",
    "# Get feature names from the pipeline's preprocessor. This list is the full feature list\n",
    "feature_names = best_model.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "# Convert to df\n",
    "X_test_transformed_df = pd.DataFrame(X_test_transformed, columns=feature_names)\n",
    "\n",
    "#reset indicies\n",
    "X_test_transformed_df.reset_index(drop=True, inplace=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "# Identify the false positives\n",
    "false_positives_mask = (y_pred == 1) & (y_test == 0)\n",
    "\n",
    "# Filter X_test for false positives\n",
    "false_positives_df = X_test_transformed_df[false_positives_mask.values]\n",
    "\n",
    "# Store the possible problem features\n",
    "possible_problem_features = []\n",
    "\n",
    "for column in false_positives_df.columns:\n",
    "        \n",
    "    # Get the most common occurring value in the column \n",
    "    mode_value = false_positives_df[column].mode()[0]\n",
    "    mode_freq = (false_positives_df[column] == mode_value).mean()\n",
    "    \n",
    "    # If the column is made up primarily of a single value\n",
    "    if mode_freq > 0.9:\n",
    "\n",
    "        possible_problem_features.append(column)\n",
    "        \n",
    "        print(f\"Feature: {column}\")\n",
    "        print(f\"Mode value: {mode_value}\")\n",
    "        print(f\"Frequency of mode: {mode_freq}\")\n",
    "        print('-'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aa8e09-a129-4b93-8784-0949237bb8f2",
   "metadata": {},
   "source": [
    "We can see that for false positives, the onehot encoded category `cat__home_ownership_ANY` as example, is primarily composed of 0 values, and occurs with a frequency of close to 100% among false positives. Meaning, the majority of false positives did not have some form of home ownership. \n",
    "\n",
    "This alone does not mean the `home_ownership` feature is a problem for the linear model. It's distribution should be analyzed across the full X_test dataset and then compared to the false positive set in order discern any discrepancy between the two. The other onehot encoded categorical values should also be considered along with the \"ANY\" flag.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36fcb49-a2c4-4174-910c-b5c6a41a565a",
   "metadata": {},
   "source": [
    "***PDP graphs***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d5b8d-3500-49fc-8553-0350a337dba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index of the feature name\n",
    "feature_index = list(feature_names_transformed).index('cat__home_ownership_MORTGAGE')\n",
    "\n",
    "# Plot the PDP using the index\n",
    "display = PartialDependenceDisplay.from_estimator(\n",
    "    best_model, X_test_transformed, [feature_index], grid_resolution=50\n",
    ")\n",
    "display.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fbed70-98a8-4f72-ba5b-8134f872cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure X_test is transformed using the same preprocessing steps as the training data\n",
    "X_test_transformed = best_model.named_steps['preprocessor'].transform(X_test)\n",
    "\n",
    "# Double-check the transformed feature names\n",
    "feature_names_transformed = best_model.named_steps['preprocessor'].get_feature_names_out()\n",
    "print(feature_names_transformed)\n",
    "\n",
    "# Now, verify the presence of the feature again\n",
    "if 'cat__home_ownership_MORTGAGE' in feature_names_transformed:\n",
    "    print('It is in')\n",
    "    # If present, plot the PDP\n",
    "    display = PartialDependenceDisplay.from_estimator(\n",
    "        best_model, X_test_transformed, ['home_ownership'], grid_resolution=50\n",
    "    )\n",
    "    display.plot()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"'cat__home_ownership_MORTGAGE' is not found in the transformed features.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1342fae2-749b-4aa2-8dde-5b2b3dcd3397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure possible_problem_features only contains names that are in feature_names\n",
    "possible_problem_features = [f for f in possible_problem_features if f in feature_names]\n",
    "print(possible_problem_features)\n",
    "\n",
    "# Now you can iterate over the possible problem features and plot the PDPs\n",
    "for feature_name in possible_problem_features:\n",
    "        # Create the display object\n",
    "        display = PartialDependenceDisplay.from_estimator(\n",
    "            best_model, X_test_transformed, [feature_name], grid_resolution=50\n",
    "        )\n",
    "        # Plot the partial dependence\n",
    "        display.plot()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c6a400-1ba5-4888-a5b8-0998d2766f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "for feature in majority_value_columns:\n",
    "    feature_name = feature\n",
    "    # Create the display object\n",
    "    display = PartialDependenceDisplay.from_estimator(best_model, X_test, [feature_name])\n",
    "    # Plot the partial dependence\n",
    "    display.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682baf2-b72b-447e-a118-34fd806959d6",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ece38c-0544-46c4-9fa5-7d8262131b85",
   "metadata": {},
   "source": [
    "***WRITE THE RESULTS OF BEST OUTCOME TO FILE OR CSV TO BE COMPARED WITH LATER***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13798419-ca23-4f3e-9ed9-585185b29a72",
   "metadata": {},
   "source": [
    "The logistic regression model performed quite well considering its explainability and ease of use. We achieved a 66% and 65% accuracy on our baseline model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loans_capstone",
   "language": "python",
   "name": "loans_capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
