{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7737e2f6-ee61-4939-b1e9-4d86244412cc",
   "metadata": {},
   "source": [
    "# Base Model: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb8d57e-f8e2-438f-ac20-e55f148bb495",
   "metadata": {},
   "source": [
    "## Date: Nov 9, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256c83e4-ba8b-41a0-a026-0ebaeec4d1b3",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98074a2c-ee81-473a-a32e-34eb6dc688d8",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae00c66-f311-4423-ad64-b7943e0178cf",
   "metadata": {},
   "source": [
    "In this notebook, we will establish a baseline classification model using Logistic Regression. Log reg is a good baseline as it is one of the simplist classification models, and offers high explainability compared to its counterparts. Furthermore, it can be far less computationally heavy.   \n",
    "After the data is read in, some final leaky columns are dropped, and then assumptions are established and checked. These were all completed aside from multicollinearity due to a bug when using VIF. After the assumptions have been checked, a model was run and evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf6673d-f070-4f0b-96bd-706e29346538",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8497c5-d4a9-4dea-b86e-238a39652efb",
   "metadata": {},
   "source": [
    "### Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7801100c-f90c-4284-b341-38ad75aa8570",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)\n",
    "   - [Table of Contents](#Table-of-contents)\n",
    "   - [Import Librarys](#Import-Librarys)\n",
    "   - [Data Dictionary](#Data-Dictionary)\n",
    "   - [Define Functions](#Define-Functions)\n",
    "   - [Load the data](#Load-the-data)\n",
    "3. [Logistic Regression Model](#Logistic-Regression-Model)\n",
    "   - [Feature Engineering](#Feature-Engineering)\n",
    "   - [Assumptions](#Assumptions)\n",
    "   - [Modelling](#Modelling)\n",
    "   - [Evaluation](#Evaluation)\n",
    "8. [Conclusion](#Conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8125e564-2b37-4bdb-a886-123ed781112a",
   "metadata": {},
   "source": [
    "### Import Librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b2b331-0acd-4951-b043-a32df4e84c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cbeb4b-1a60-4ed0-860d-f4b52aa2d41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25946e1f-a975-4d39-a161-bd37d2646a21",
   "metadata": {},
   "source": [
    "### Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6636a5f3-646d-4698-89d9-cb0c4c780f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ff308-4904-4076-a245-40e57fcd7c29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pathlib is used to ensure compatibility across operating systems\n",
    "try:\n",
    "    data_destination = Path('../Data/Lending_club/Lending Club Data Dictionary Approved.csv')\n",
    "    dict_df = pd.read_csv(data_destination, encoding='ISO-8859-1')\n",
    "    display(dict_df.iloc[:,0:2])\n",
    "except FileNotFoundError as e:\n",
    "    print(e.args[1])\n",
    "    print('Check file location')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249e4929-f85e-492b-a8ba-b9a4e5772d02",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e07d49-3123-43f8-92e6-d650d28887d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the relative path to the file\n",
    "parquet_file_path = Path('../Data/Lending_club/Cleaned')\n",
    "\n",
    "try:\n",
    "    # Read the parquet file\n",
    "    loans_df = pd.read_parquet(parquet_file_path)\n",
    "except FileNotFoundError as e:\n",
    "    print(e.args[1])\n",
    "    print('Check file location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a0c361-0ecf-4d29-b758-8f938f49c20e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loans_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bc45ce-c611-4dd2-93db-d3090f11b343",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c95760f-a9ab-428a-bb0f-a4b864a7e813",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5a4cde-668c-4c00-ac3b-32c8c19618ad",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30503347-240e-4774-b315-06d298769ee5",
   "metadata": {},
   "source": [
    "***There is some data prep still left from EDA.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d664523-5a00-4744-8ae7-19ec8860916b",
   "metadata": {},
   "source": [
    "Drop any Leaky columns left over from EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3d9d61-dbed-4eba-9416-9751096f3397",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_df.drop(columns=['funded_amnt', 'funded_amnt_inv', 'chargeoff_within_12_mths', 'delinq_amnt'], inplace=True)\n",
    "\n",
    "# Also any categorical columns with too many categories for one hot encoding\n",
    "loans_df.drop(columns=['issue_d', 'earliest_cr_line'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b906ce-2107-4eb7-9952-eda326cc0e93",
   "metadata": {},
   "source": [
    "Map Successful loans to 1, and Defaulted or Charged Off loans to 0 in our target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb407dc1-7fd0-41e3-b6a3-06c56aa9a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_df['loan_status'] = loans_df['loan_status'].apply(lambda x: 1 if x == 'Fully Paid' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731264c4-c62d-43a6-bd94-5deb112bf41b",
   "metadata": {},
   "source": [
    "### Assumptions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b262f00-2669-4ea2-b17b-058e6083bc9d",
   "metadata": {},
   "source": [
    "Before we can start modeling, some base assumptions must be met in order to use a log reg model.   \n",
    "These include:  \n",
    "* Binary Outcome. The dependent variable is a binary (met)\n",
    "* Independence. The observations should be independent. It is reasonable to assume independence here. Without identifiable information, there is not way of knowing from the dataset whether a borrower has applied for multiple loans as each loan id is unique.\n",
    "* No collinearity / multicollinearity. This will be checked\n",
    "* Large sample size. (met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50b4271-3a5a-45a8-94f9-19bab920a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error running vif to check for multicollinearity. This will be fixed for the next iteration\n",
    "\n",
    "# Check for multicollinearity using Variance Inflation Factor (VIF)\n",
    "#vif_data = pd.DataFrame()\n",
    "#vif_data['feature'] = X.columns\n",
    "#vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "\n",
    "# Drop features with high VIF\n",
    "#high_vif_columns = vif_data[vif_data['VIF'] > 9]['feature'].tolist()\n",
    "#X = X.drop(high_vif_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780db858-d49e-4d4f-9704-ac3e9102c8a5",
   "metadata": {},
   "source": [
    "***Collinearity***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1278d4f2-f029-4cbe-8b79-0df3ee80241a",
   "metadata": {},
   "source": [
    "We will plot a correlation heatmap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf841cc-3627-4f9b-b30f-44c372bd7f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the numeric columns for the correlation matrix\n",
    "numeric_df = loans_df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr = numeric_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(corr, mask=mask, cmap='coolwarm', vmax=1, vmin=-1, center=0,\n",
    "            square=True, linewidths=.5, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69dddf0-5133-4b01-800c-4dc519f7b31c",
   "metadata": {},
   "source": [
    "We can see that `open_acc` has high correlation with many other features, as well as `tot_cur_bal` and `num_sats`. We can drop these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc86bd7-bacd-43b6-a3fd-3a5bd4fe9007",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_df.drop(columns=['open_acc', 'tot_cur_bal', 'num_sats', 'num_accts_ever_120_pd'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c583198-629f-4c67-a2cd-56ae43133551",
   "metadata": {},
   "source": [
    "We can now create dummy variables for our categorical variables and split the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c767a32e-3e79-4bdc-82b7-2dc6f3434252",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert categorical variables to dummy variables\n",
    "categorical_cols = loans_df.select_dtypes(include=['object']).columns\n",
    "loans_df = pd.get_dummies(loans_df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Split the data\n",
    "X = loans_df.drop(columns=['loan_status'], inplace=False)\n",
    "y = loans_df['loan_status']\n",
    "\n",
    "# Split into train and test sets. Stratify to account for imbalance\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2d5939-a022-4f6c-ae5e-b6dbdaf60f17",
   "metadata": {},
   "source": [
    "We can now run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca12d4-961c-4916-995f-2d86f507f215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initializing and training the logistic regression model\n",
    "log_reg = LogisticRegression(random_state=1,\n",
    "                             solver='lbfgs', \n",
    "                             max_iter=3000, \n",
    "                             verbose=2, #output while the model runs\n",
    "                             n_jobs=2, #use 2 cpu cores\n",
    "                             class_weight='balanced') #weight the class to counter more frequent class\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Making predictions on the test data using the trained model\n",
    "y_pred = log_reg.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe9c2c0-db6d-4c60-a2f2-e8f4dc2cb041",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed01482-8aee-4888-8ac6-5c91a21cb0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring the model on both train and test data\n",
    "train_score = log_reg.score(X_train_scaled, y_train)\n",
    "test_score = log_reg.score(X_test_scaled, y_test)\n",
    "print(f'Score on train: {train_score}')\n",
    "print(f'Score on test: {test_score}')\n",
    "\n",
    "# Evaluating the model with confusion matrix and a classification report\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "display(conf_matrix)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c8a335-e72c-427d-b90c-26162d199f39",
   "metadata": {},
   "source": [
    "Our model has approximately 66% accuracy on the train set. The score between the train and test set are relatively close, however there might be some overfitting since the accuracy on the test set is lower. We can combat this by passing a C value to regularize the model. The model is better at predicting successful loans, likely due to the data imbalance, with class 1 having a higher precision and recall. Of all the actual fully paid loans, 66% were correctly predicted, with 64% of all failed loans correctly predicted. Due to the cost of false positives in this context (predicting a loan to be repaid when it will be defaulted on), a higher precision would be preferred. However, this comes at the cost of a lower recall, meaning good lending opportunities could be missed (false negatives).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1142342-aedc-478c-a0b2-5a0aca3863a3",
   "metadata": {},
   "source": [
    "We can explore which features were most used useful in the prediction be inspecting their weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb9e7eb-d851-4be5-abdc-82c43a13b631",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assuming 'model' is your fitted Logistic Regression model\n",
    "feature_weights = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': log_reg.coef_[0]\n",
    "})\n",
    "\n",
    "# Sort the features by the absolute value of their coefficient\n",
    "feature_weights = feature_weights.sort_values(by='Coefficient', ascending=True)\n",
    "\n",
    "# Display the feature weights\n",
    "feature_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682baf2-b72b-447e-a118-34fd806959d6",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13798419-ca23-4f3e-9ed9-585185b29a72",
   "metadata": {},
   "source": [
    "The logistic regression model performed quite well considering its explainability and ease of use. We achieved a 66% and 65% accuracy on our baseline model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loans_capstone",
   "language": "python",
   "name": "loans_capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
